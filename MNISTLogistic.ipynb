{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.9089, Accuracy: 0.8075\n",
      "Epoch [2/20], Loss: 0.5489, Accuracy: 0.8600\n",
      "Epoch [3/20], Loss: 0.4464, Accuracy: 0.8814\n",
      "Epoch [4/20], Loss: 0.3971, Accuracy: 0.8921\n",
      "Epoch [5/20], Loss: 0.3687, Accuracy: 0.8980\n",
      "Epoch [6/20], Loss: 0.3502, Accuracy: 0.9019\n",
      "Epoch [7/20], Loss: 0.3353, Accuracy: 0.9059\n",
      "Epoch [8/20], Loss: 0.3247, Accuracy: 0.9085\n",
      "Epoch [9/20], Loss: 0.3146, Accuracy: 0.9110\n",
      "Epoch [10/20], Loss: 0.3069, Accuracy: 0.9139\n",
      "Epoch [11/20], Loss: 0.2996, Accuracy: 0.9162\n",
      "Epoch [12/20], Loss: 0.2931, Accuracy: 0.9178\n",
      "Epoch [13/20], Loss: 0.2869, Accuracy: 0.9198\n",
      "Epoch [14/20], Loss: 0.2817, Accuracy: 0.9209\n",
      "Epoch [15/20], Loss: 0.2765, Accuracy: 0.9228\n",
      "Epoch [16/20], Loss: 0.2717, Accuracy: 0.9241\n",
      "Epoch [17/20], Loss: 0.2672, Accuracy: 0.9250\n",
      "Epoch [18/20], Loss: 0.2627, Accuracy: 0.9273\n",
      "Epoch [19/20], Loss: 0.2585, Accuracy: 0.9281\n",
      "Epoch [20/20], Loss: 0.2548, Accuracy: 0.9290\n",
      "Epoch [1/20], Loss: 0.2256, Accuracy: 0.9367\n",
      "Epoch [2/20], Loss: 0.2014, Accuracy: 0.9431\n",
      "Epoch [3/20], Loss: 0.1820, Accuracy: 0.9482\n",
      "Epoch [4/20], Loss: 0.1619, Accuracy: 0.9535\n",
      "Epoch [5/20], Loss: 0.1499, Accuracy: 0.9571\n",
      "Epoch [6/20], Loss: 0.1402, Accuracy: 0.9606\n",
      "Epoch [7/20], Loss: 0.1333, Accuracy: 0.9610\n",
      "Epoch [8/20], Loss: 0.1230, Accuracy: 0.9642\n",
      "Epoch [9/20], Loss: 0.1122, Accuracy: 0.9673\n",
      "Epoch [10/20], Loss: 0.1046, Accuracy: 0.9701\n",
      "Epoch [11/20], Loss: 0.0976, Accuracy: 0.9719\n",
      "Epoch [12/20], Loss: 0.0933, Accuracy: 0.9737\n",
      "Epoch [13/20], Loss: 0.0919, Accuracy: 0.9730\n",
      "Epoch [14/20], Loss: 0.0910, Accuracy: 0.9736\n",
      "Epoch [15/20], Loss: 0.0814, Accuracy: 0.9767\n",
      "Epoch [16/20], Loss: 0.0803, Accuracy: 0.9768\n",
      "Epoch [17/20], Loss: 0.0751, Accuracy: 0.9784\n",
      "Epoch [18/20], Loss: 0.0768, Accuracy: 0.9781\n",
      "Epoch [19/20], Loss: 0.0698, Accuracy: 0.9801\n",
      "Epoch [20/20], Loss: 0.0700, Accuracy: 0.9801\n",
      "Epoch [1/20], Loss: 0.1063, Accuracy: 0.9674\n",
      "Epoch [2/20], Loss: 0.1005, Accuracy: 0.9688\n",
      "Epoch [3/20], Loss: 0.0849, Accuracy: 0.9735\n",
      "Epoch [4/20], Loss: 0.0942, Accuracy: 0.9707\n",
      "Epoch [5/20], Loss: 0.0904, Accuracy: 0.9717\n",
      "Epoch [6/20], Loss: 0.0658, Accuracy: 0.9794\n",
      "Epoch [7/20], Loss: 0.0639, Accuracy: 0.9798\n",
      "Epoch [8/20], Loss: 0.0551, Accuracy: 0.9831\n",
      "Epoch [9/20], Loss: 0.0628, Accuracy: 0.9796\n",
      "Epoch [10/20], Loss: 0.0548, Accuracy: 0.9825\n",
      "Epoch [11/20], Loss: 0.0574, Accuracy: 0.9808\n",
      "Epoch [12/20], Loss: 0.0459, Accuracy: 0.9854\n",
      "Epoch [13/20], Loss: 0.0553, Accuracy: 0.9820\n",
      "Epoch [14/20], Loss: 0.0483, Accuracy: 0.9838\n",
      "Epoch [15/20], Loss: 0.0457, Accuracy: 0.9856\n",
      "Epoch [16/20], Loss: 0.0409, Accuracy: 0.9868\n",
      "Epoch [17/20], Loss: 0.0462, Accuracy: 0.9851\n",
      "Epoch [18/20], Loss: 0.0422, Accuracy: 0.9861\n",
      "Epoch [19/20], Loss: 0.0421, Accuracy: 0.9867\n",
      "Epoch [20/20], Loss: 0.0421, Accuracy: 0.9853\n",
      "Epoch [1/20], Loss: 0.2000, Accuracy: 0.9393\n",
      "Epoch [2/20], Loss: 0.2067, Accuracy: 0.9331\n",
      "Epoch [3/20], Loss: 0.1397, Accuracy: 0.9550\n",
      "Epoch [4/20], Loss: 0.1517, Accuracy: 0.9507\n",
      "Epoch [5/20], Loss: 0.1148, Accuracy: 0.9624\n",
      "Epoch [6/20], Loss: 0.1168, Accuracy: 0.9636\n",
      "Epoch [7/20], Loss: 0.1079, Accuracy: 0.9651\n",
      "Epoch [8/20], Loss: 0.1124, Accuracy: 0.9649\n",
      "Epoch [9/20], Loss: 0.0874, Accuracy: 0.9729\n",
      "Epoch [10/20], Loss: 0.0815, Accuracy: 0.9753\n",
      "Epoch [11/20], Loss: 0.1037, Accuracy: 0.9680\n",
      "Epoch [12/20], Loss: 0.0755, Accuracy: 0.9771\n",
      "Epoch [13/20], Loss: 0.0839, Accuracy: 0.9732\n",
      "Epoch [14/20], Loss: 0.1012, Accuracy: 0.9681\n",
      "Epoch [15/20], Loss: 0.0823, Accuracy: 0.9734\n",
      "Epoch [16/20], Loss: 0.0817, Accuracy: 0.9739\n",
      "Epoch [17/20], Loss: 0.0797, Accuracy: 0.9741\n",
      "Epoch [18/20], Loss: 0.0845, Accuracy: 0.9739\n",
      "Epoch [19/20], Loss: 0.0837, Accuracy: 0.9751\n",
      "Epoch [20/20], Loss: 0.0692, Accuracy: 0.9780\n",
      "Epoch [1/20], Loss: 0.9553, Accuracy: 0.8165\n",
      "Epoch [2/20], Loss: 0.5480, Accuracy: 0.8667\n",
      "Epoch [3/20], Loss: 0.4433, Accuracy: 0.8825\n",
      "Epoch [4/20], Loss: 0.3959, Accuracy: 0.8914\n",
      "Epoch [5/20], Loss: 0.3674, Accuracy: 0.8979\n",
      "Epoch [6/20], Loss: 0.3488, Accuracy: 0.9019\n",
      "Epoch [7/20], Loss: 0.3345, Accuracy: 0.9053\n",
      "Epoch [8/20], Loss: 0.3238, Accuracy: 0.9085\n",
      "Epoch [9/20], Loss: 0.3139, Accuracy: 0.9113\n",
      "Epoch [10/20], Loss: 0.3062, Accuracy: 0.9137\n",
      "Epoch [11/20], Loss: 0.2990, Accuracy: 0.9148\n",
      "Epoch [12/20], Loss: 0.2926, Accuracy: 0.9171\n",
      "Epoch [13/20], Loss: 0.2870, Accuracy: 0.9182\n",
      "Epoch [14/20], Loss: 0.2815, Accuracy: 0.9202\n",
      "Epoch [15/20], Loss: 0.2763, Accuracy: 0.9220\n",
      "Epoch [16/20], Loss: 0.2719, Accuracy: 0.9238\n",
      "Epoch [17/20], Loss: 0.2667, Accuracy: 0.9246\n",
      "Epoch [18/20], Loss: 0.2626, Accuracy: 0.9256\n",
      "Epoch [19/20], Loss: 0.2578, Accuracy: 0.9273\n",
      "Epoch [20/20], Loss: 0.2535, Accuracy: 0.9282\n",
      "Epoch [1/20], Loss: 0.2226, Accuracy: 0.9364\n",
      "Epoch [2/20], Loss: 0.1977, Accuracy: 0.9438\n",
      "Epoch [3/20], Loss: 0.1770, Accuracy: 0.9501\n",
      "Epoch [4/20], Loss: 0.1613, Accuracy: 0.9545\n",
      "Epoch [5/20], Loss: 0.1495, Accuracy: 0.9579\n",
      "Epoch [6/20], Loss: 0.1357, Accuracy: 0.9622\n",
      "Epoch [7/20], Loss: 0.1321, Accuracy: 0.9618\n",
      "Epoch [8/20], Loss: 0.1250, Accuracy: 0.9644\n",
      "Epoch [9/20], Loss: 0.1114, Accuracy: 0.9683\n",
      "Epoch [10/20], Loss: 0.1111, Accuracy: 0.9684\n",
      "Epoch [11/20], Loss: 0.1062, Accuracy: 0.9693\n",
      "Epoch [12/20], Loss: 0.0968, Accuracy: 0.9723\n",
      "Epoch [13/20], Loss: 0.0960, Accuracy: 0.9720\n",
      "Epoch [14/20], Loss: 0.0894, Accuracy: 0.9744\n",
      "Epoch [15/20], Loss: 0.0869, Accuracy: 0.9752\n",
      "Epoch [16/20], Loss: 0.0845, Accuracy: 0.9757\n",
      "Epoch [17/20], Loss: 0.0831, Accuracy: 0.9755\n",
      "Epoch [18/20], Loss: 0.0778, Accuracy: 0.9774\n",
      "Epoch [19/20], Loss: 0.0749, Accuracy: 0.9784\n",
      "Epoch [20/20], Loss: 0.0698, Accuracy: 0.9803\n",
      "Epoch [1/20], Loss: 0.1028, Accuracy: 0.9686\n",
      "Epoch [2/20], Loss: 0.1393, Accuracy: 0.9552\n",
      "Epoch [3/20], Loss: 0.0812, Accuracy: 0.9742\n",
      "Epoch [4/20], Loss: 0.0743, Accuracy: 0.9762\n",
      "Epoch [5/20], Loss: 0.0681, Accuracy: 0.9788\n",
      "Epoch [6/20], Loss: 0.0684, Accuracy: 0.9788\n",
      "Epoch [7/20], Loss: 0.0791, Accuracy: 0.9743\n",
      "Epoch [8/20], Loss: 0.0532, Accuracy: 0.9831\n",
      "Epoch [9/20], Loss: 0.0533, Accuracy: 0.9832\n",
      "Epoch [10/20], Loss: 0.0547, Accuracy: 0.9821\n",
      "Epoch [11/20], Loss: 0.0492, Accuracy: 0.9843\n",
      "Epoch [12/20], Loss: 0.0573, Accuracy: 0.9823\n",
      "Epoch [13/20], Loss: 0.0572, Accuracy: 0.9810\n",
      "Epoch [14/20], Loss: 0.0463, Accuracy: 0.9854\n",
      "Epoch [15/20], Loss: 0.0619, Accuracy: 0.9792\n",
      "Epoch [16/20], Loss: 0.0394, Accuracy: 0.9865\n",
      "Epoch [17/20], Loss: 0.0393, Accuracy: 0.9878\n",
      "Epoch [18/20], Loss: 0.0323, Accuracy: 0.9900\n",
      "Epoch [19/20], Loss: 0.0467, Accuracy: 0.9841\n",
      "Epoch [20/20], Loss: 0.0362, Accuracy: 0.9880\n",
      "Epoch [1/20], Loss: 0.1480, Accuracy: 0.9561\n",
      "Epoch [2/20], Loss: 0.1115, Accuracy: 0.9655\n",
      "Epoch [3/20], Loss: 0.1161, Accuracy: 0.9649\n",
      "Epoch [4/20], Loss: 0.1464, Accuracy: 0.9536\n",
      "Epoch [5/20], Loss: 0.0948, Accuracy: 0.9695\n",
      "Epoch [6/20], Loss: 0.0795, Accuracy: 0.9748\n",
      "Epoch [7/20], Loss: 0.0825, Accuracy: 0.9740\n",
      "Epoch [8/20], Loss: 0.0817, Accuracy: 0.9732\n",
      "Epoch [9/20], Loss: 0.0801, Accuracy: 0.9753\n",
      "Epoch [10/20], Loss: 0.0764, Accuracy: 0.9751\n",
      "Epoch [11/20], Loss: 0.0741, Accuracy: 0.9769\n",
      "Epoch [12/20], Loss: 0.0683, Accuracy: 0.9780\n",
      "Epoch [13/20], Loss: 0.0663, Accuracy: 0.9779\n",
      "Epoch [14/20], Loss: 0.0669, Accuracy: 0.9782\n",
      "Epoch [15/20], Loss: 0.0590, Accuracy: 0.9808\n",
      "Epoch [16/20], Loss: 0.0718, Accuracy: 0.9780\n",
      "Epoch [17/20], Loss: 0.0677, Accuracy: 0.9776\n",
      "Epoch [18/20], Loss: 0.0522, Accuracy: 0.9828\n",
      "Epoch [19/20], Loss: 0.0601, Accuracy: 0.9798\n",
      "Epoch [20/20], Loss: 0.0468, Accuracy: 0.9847\n",
      "Epoch [1/20], Loss: 0.9898, Accuracy: 0.8036\n",
      "Epoch [2/20], Loss: 0.6428, Accuracy: 0.8552\n",
      "Epoch [3/20], Loss: 0.5122, Accuracy: 0.8755\n",
      "Epoch [4/20], Loss: 0.4453, Accuracy: 0.8850\n",
      "Epoch [5/20], Loss: 0.4047, Accuracy: 0.8924\n",
      "Epoch [6/20], Loss: 0.3769, Accuracy: 0.8980\n",
      "Epoch [7/20], Loss: 0.3563, Accuracy: 0.9021\n",
      "Epoch [8/20], Loss: 0.3403, Accuracy: 0.9055\n",
      "Epoch [9/20], Loss: 0.3271, Accuracy: 0.9083\n",
      "Epoch [10/20], Loss: 0.3162, Accuracy: 0.9110\n",
      "Epoch [11/20], Loss: 0.3065, Accuracy: 0.9136\n",
      "Epoch [12/20], Loss: 0.2981, Accuracy: 0.9155\n",
      "Epoch [13/20], Loss: 0.2909, Accuracy: 0.9178\n",
      "Epoch [14/20], Loss: 0.2839, Accuracy: 0.9194\n",
      "Epoch [15/20], Loss: 0.2778, Accuracy: 0.9210\n",
      "Epoch [16/20], Loss: 0.2722, Accuracy: 0.9226\n",
      "Epoch [17/20], Loss: 0.2669, Accuracy: 0.9242\n",
      "Epoch [18/20], Loss: 0.2618, Accuracy: 0.9253\n",
      "Epoch [19/20], Loss: 0.2574, Accuracy: 0.9265\n",
      "Epoch [20/20], Loss: 0.2528, Accuracy: 0.9277\n",
      "Epoch [1/20], Loss: 0.2235, Accuracy: 0.9366\n",
      "Epoch [2/20], Loss: 0.1987, Accuracy: 0.9426\n",
      "Epoch [3/20], Loss: 0.1801, Accuracy: 0.9483\n",
      "Epoch [4/20], Loss: 0.1693, Accuracy: 0.9515\n",
      "Epoch [5/20], Loss: 0.1564, Accuracy: 0.9553\n",
      "Epoch [6/20], Loss: 0.1451, Accuracy: 0.9584\n",
      "Epoch [7/20], Loss: 0.1379, Accuracy: 0.9601\n",
      "Epoch [8/20], Loss: 0.1300, Accuracy: 0.9625\n",
      "Epoch [9/20], Loss: 0.1229, Accuracy: 0.9653\n",
      "Epoch [10/20], Loss: 0.1177, Accuracy: 0.9668\n",
      "Epoch [11/20], Loss: 0.1129, Accuracy: 0.9688\n",
      "Epoch [12/20], Loss: 0.1094, Accuracy: 0.9691\n",
      "Epoch [13/20], Loss: 0.1046, Accuracy: 0.9705\n",
      "Epoch [14/20], Loss: 0.1002, Accuracy: 0.9720\n",
      "Epoch [15/20], Loss: 0.0967, Accuracy: 0.9730\n",
      "Epoch [16/20], Loss: 0.0931, Accuracy: 0.9742\n",
      "Epoch [17/20], Loss: 0.0924, Accuracy: 0.9744\n",
      "Epoch [18/20], Loss: 0.0880, Accuracy: 0.9761\n",
      "Epoch [19/20], Loss: 0.0853, Accuracy: 0.9768\n",
      "Epoch [20/20], Loss: 0.0822, Accuracy: 0.9780\n",
      "Epoch [1/20], Loss: 0.0919, Accuracy: 0.9726\n",
      "Epoch [2/20], Loss: 0.0855, Accuracy: 0.9735\n",
      "Epoch [3/20], Loss: 0.0757, Accuracy: 0.9774\n",
      "Epoch [4/20], Loss: 0.0705, Accuracy: 0.9790\n",
      "Epoch [5/20], Loss: 0.0664, Accuracy: 0.9798\n",
      "Epoch [6/20], Loss: 0.0563, Accuracy: 0.9835\n",
      "Epoch [7/20], Loss: 0.0578, Accuracy: 0.9828\n",
      "Epoch [8/20], Loss: 0.0557, Accuracy: 0.9835\n",
      "Epoch [9/20], Loss: 0.0543, Accuracy: 0.9838\n",
      "Epoch [10/20], Loss: 0.0590, Accuracy: 0.9813\n",
      "Epoch [11/20], Loss: 0.0489, Accuracy: 0.9850\n",
      "Epoch [12/20], Loss: 0.0491, Accuracy: 0.9853\n",
      "Epoch [13/20], Loss: 0.0413, Accuracy: 0.9885\n",
      "Epoch [14/20], Loss: 0.0400, Accuracy: 0.9889\n",
      "Epoch [15/20], Loss: 0.0396, Accuracy: 0.9888\n",
      "Epoch [16/20], Loss: 0.0380, Accuracy: 0.9894\n",
      "Epoch [17/20], Loss: 0.0363, Accuracy: 0.9902\n",
      "Epoch [18/20], Loss: 0.0332, Accuracy: 0.9905\n",
      "Epoch [19/20], Loss: 0.0355, Accuracy: 0.9896\n",
      "Epoch [20/20], Loss: 0.0380, Accuracy: 0.9887\n",
      "Epoch [1/20], Loss: 0.0687, Accuracy: 0.9765\n",
      "Epoch [2/20], Loss: 0.0639, Accuracy: 0.9785\n",
      "Epoch [3/20], Loss: 0.0622, Accuracy: 0.9793\n",
      "Epoch [4/20], Loss: 0.0516, Accuracy: 0.9825\n",
      "Epoch [5/20], Loss: 0.0503, Accuracy: 0.9831\n",
      "Epoch [6/20], Loss: 0.0546, Accuracy: 0.9811\n",
      "Epoch [7/20], Loss: 0.0429, Accuracy: 0.9863\n",
      "Epoch [8/20], Loss: 0.0471, Accuracy: 0.9842\n",
      "Epoch [9/20], Loss: 0.0473, Accuracy: 0.9847\n",
      "Epoch [10/20], Loss: 0.0414, Accuracy: 0.9861\n",
      "Epoch [11/20], Loss: 0.0434, Accuracy: 0.9861\n",
      "Epoch [12/20], Loss: 0.0358, Accuracy: 0.9879\n",
      "Epoch [13/20], Loss: 0.0313, Accuracy: 0.9904\n",
      "Epoch [14/20], Loss: 0.0529, Accuracy: 0.9829\n",
      "Epoch [15/20], Loss: 0.0318, Accuracy: 0.9903\n",
      "Epoch [16/20], Loss: 0.0344, Accuracy: 0.9886\n",
      "Epoch [17/20], Loss: 0.0254, Accuracy: 0.9921\n",
      "Epoch [18/20], Loss: 0.0220, Accuracy: 0.9938\n",
      "Epoch [19/20], Loss: 0.0300, Accuracy: 0.9906\n",
      "Epoch [20/20], Loss: 0.0253, Accuracy: 0.9925\n",
      "Epoch [1/20], Loss: 2.1424, Accuracy: 0.5468\n",
      "Epoch [2/20], Loss: 1.8775, Accuracy: 0.6696\n",
      "Epoch [3/20], Loss: 1.5523, Accuracy: 0.7061\n",
      "Epoch [4/20], Loss: 1.2771, Accuracy: 0.7512\n",
      "Epoch [5/20], Loss: 1.0760, Accuracy: 0.7902\n",
      "Epoch [6/20], Loss: 0.9319, Accuracy: 0.8108\n",
      "Epoch [7/20], Loss: 0.8266, Accuracy: 0.8235\n",
      "Epoch [8/20], Loss: 0.7473, Accuracy: 0.8382\n",
      "Epoch [9/20], Loss: 0.6859, Accuracy: 0.8483\n",
      "Epoch [10/20], Loss: 0.6373, Accuracy: 0.8550\n",
      "Epoch [11/20], Loss: 0.5976, Accuracy: 0.8626\n",
      "Epoch [12/20], Loss: 0.5647, Accuracy: 0.8673\n",
      "Epoch [13/20], Loss: 0.5370, Accuracy: 0.8723\n",
      "Epoch [14/20], Loss: 0.5135, Accuracy: 0.8759\n",
      "Epoch [15/20], Loss: 0.4931, Accuracy: 0.8793\n",
      "Epoch [16/20], Loss: 0.4754, Accuracy: 0.8824\n",
      "Epoch [17/20], Loss: 0.4600, Accuracy: 0.8846\n",
      "Epoch [18/20], Loss: 0.4462, Accuracy: 0.8873\n",
      "Epoch [19/20], Loss: 0.4340, Accuracy: 0.8894\n",
      "Epoch [20/20], Loss: 0.4230, Accuracy: 0.8908\n",
      "Epoch [1/20], Loss: 0.3545, Accuracy: 0.9035\n",
      "Epoch [2/20], Loss: 0.3190, Accuracy: 0.9116\n",
      "Epoch [3/20], Loss: 0.2934, Accuracy: 0.9180\n",
      "Epoch [4/20], Loss: 0.2758, Accuracy: 0.9234\n",
      "Epoch [5/20], Loss: 0.2618, Accuracy: 0.9266\n",
      "Epoch [6/20], Loss: 0.2493, Accuracy: 0.9290\n",
      "Epoch [7/20], Loss: 0.2388, Accuracy: 0.9327\n",
      "Epoch [8/20], Loss: 0.2294, Accuracy: 0.9352\n",
      "Epoch [9/20], Loss: 0.2213, Accuracy: 0.9372\n",
      "Epoch [10/20], Loss: 0.2136, Accuracy: 0.9393\n",
      "Epoch [11/20], Loss: 0.2067, Accuracy: 0.9410\n",
      "Epoch [12/20], Loss: 0.2005, Accuracy: 0.9429\n",
      "Epoch [13/20], Loss: 0.1947, Accuracy: 0.9447\n",
      "Epoch [14/20], Loss: 0.1899, Accuracy: 0.9466\n",
      "Epoch [15/20], Loss: 0.1851, Accuracy: 0.9469\n",
      "Epoch [16/20], Loss: 0.1800, Accuracy: 0.9493\n",
      "Epoch [17/20], Loss: 0.1760, Accuracy: 0.9499\n",
      "Epoch [18/20], Loss: 0.1714, Accuracy: 0.9515\n",
      "Epoch [19/20], Loss: 0.1680, Accuracy: 0.9522\n",
      "Epoch [20/20], Loss: 0.1644, Accuracy: 0.9534\n",
      "Epoch [1/20], Loss: 0.1513, Accuracy: 0.9570\n",
      "Epoch [2/20], Loss: 0.1420, Accuracy: 0.9593\n",
      "Epoch [3/20], Loss: 0.1314, Accuracy: 0.9633\n",
      "Epoch [4/20], Loss: 0.1252, Accuracy: 0.9640\n",
      "Epoch [5/20], Loss: 0.1239, Accuracy: 0.9641\n",
      "Epoch [6/20], Loss: 0.1102, Accuracy: 0.9689\n",
      "Epoch [7/20], Loss: 0.1049, Accuracy: 0.9709\n",
      "Epoch [8/20], Loss: 0.1001, Accuracy: 0.9721\n",
      "Epoch [9/20], Loss: 0.0970, Accuracy: 0.9727\n",
      "Epoch [10/20], Loss: 0.0923, Accuracy: 0.9747\n",
      "Epoch [11/20], Loss: 0.0904, Accuracy: 0.9751\n",
      "Epoch [12/20], Loss: 0.0861, Accuracy: 0.9762\n",
      "Epoch [13/20], Loss: 0.0835, Accuracy: 0.9775\n",
      "Epoch [14/20], Loss: 0.0803, Accuracy: 0.9772\n",
      "Epoch [15/20], Loss: 0.0779, Accuracy: 0.9784\n",
      "Epoch [16/20], Loss: 0.0773, Accuracy: 0.9786\n",
      "Epoch [17/20], Loss: 0.0729, Accuracy: 0.9799\n",
      "Epoch [18/20], Loss: 0.0733, Accuracy: 0.9800\n",
      "Epoch [19/20], Loss: 0.0708, Accuracy: 0.9802\n",
      "Epoch [20/20], Loss: 0.0665, Accuracy: 0.9824\n",
      "Epoch [1/20], Loss: 0.0719, Accuracy: 0.9794\n",
      "Epoch [2/20], Loss: 0.0669, Accuracy: 0.9806\n",
      "Epoch [3/20], Loss: 0.0607, Accuracy: 0.9831\n",
      "Epoch [4/20], Loss: 0.0675, Accuracy: 0.9807\n",
      "Epoch [5/20], Loss: 0.0585, Accuracy: 0.9836\n",
      "Epoch [6/20], Loss: 0.0559, Accuracy: 0.9842\n",
      "Epoch [7/20], Loss: 0.0550, Accuracy: 0.9845\n",
      "Epoch [8/20], Loss: 0.0514, Accuracy: 0.9860\n",
      "Epoch [9/20], Loss: 0.0493, Accuracy: 0.9865\n",
      "Epoch [10/20], Loss: 0.0507, Accuracy: 0.9860\n",
      "Epoch [11/20], Loss: 0.0478, Accuracy: 0.9869\n",
      "Epoch [12/20], Loss: 0.0443, Accuracy: 0.9886\n",
      "Epoch [13/20], Loss: 0.0427, Accuracy: 0.9890\n",
      "Epoch [14/20], Loss: 0.0444, Accuracy: 0.9879\n",
      "Epoch [15/20], Loss: 0.0433, Accuracy: 0.9884\n",
      "Epoch [16/20], Loss: 0.0381, Accuracy: 0.9908\n",
      "Epoch [17/20], Loss: 0.0388, Accuracy: 0.9896\n",
      "Epoch [18/20], Loss: 0.0428, Accuracy: 0.9882\n",
      "Epoch [19/20], Loss: 0.0346, Accuracy: 0.9915\n",
      "Epoch [20/20], Loss: 0.0345, Accuracy: 0.9917\n",
      "Epoch [1/20], Loss: 0.8270, Accuracy: 0.8228\n",
      "Epoch [2/20], Loss: 0.5301, Accuracy: 0.8681\n",
      "Epoch [3/20], Loss: 0.4386, Accuracy: 0.8823\n",
      "Epoch [4/20], Loss: 0.3954, Accuracy: 0.8912\n",
      "Epoch [5/20], Loss: 0.3702, Accuracy: 0.8958\n",
      "Epoch [6/20], Loss: 0.3529, Accuracy: 0.9002\n",
      "Epoch [7/20], Loss: 0.3409, Accuracy: 0.9024\n",
      "Epoch [8/20], Loss: 0.3301, Accuracy: 0.9060\n",
      "Epoch [9/20], Loss: 0.3228, Accuracy: 0.9083\n",
      "Epoch [10/20], Loss: 0.3158, Accuracy: 0.9101\n",
      "Epoch [11/20], Loss: 0.3105, Accuracy: 0.9116\n",
      "Epoch [12/20], Loss: 0.3045, Accuracy: 0.9138\n",
      "Epoch [13/20], Loss: 0.3001, Accuracy: 0.9156\n",
      "Epoch [14/20], Loss: 0.2956, Accuracy: 0.9168\n",
      "Epoch [15/20], Loss: 0.2917, Accuracy: 0.9173\n",
      "Epoch [16/20], Loss: 0.2875, Accuracy: 0.9191\n",
      "Epoch [17/20], Loss: 0.2846, Accuracy: 0.9197\n",
      "Epoch [18/20], Loss: 0.2801, Accuracy: 0.9211\n",
      "Epoch [19/20], Loss: 0.2769, Accuracy: 0.9222\n",
      "Epoch [20/20], Loss: 0.2741, Accuracy: 0.9232\n",
      "Epoch [1/20], Loss: 0.2495, Accuracy: 0.9293\n",
      "Epoch [2/20], Loss: 0.2191, Accuracy: 0.9384\n",
      "Epoch [3/20], Loss: 0.1974, Accuracy: 0.9451\n",
      "Epoch [4/20], Loss: 0.1767, Accuracy: 0.9503\n",
      "Epoch [5/20], Loss: 0.1629, Accuracy: 0.9540\n",
      "Epoch [6/20], Loss: 0.1489, Accuracy: 0.9587\n",
      "Epoch [7/20], Loss: 0.1430, Accuracy: 0.9597\n",
      "Epoch [8/20], Loss: 0.1314, Accuracy: 0.9636\n",
      "Epoch [9/20], Loss: 0.1248, Accuracy: 0.9656\n",
      "Epoch [10/20], Loss: 0.1193, Accuracy: 0.9665\n",
      "Epoch [11/20], Loss: 0.1121, Accuracy: 0.9689\n",
      "Epoch [12/20], Loss: 0.1081, Accuracy: 0.9692\n",
      "Epoch [13/20], Loss: 0.1028, Accuracy: 0.9717\n",
      "Epoch [14/20], Loss: 0.0973, Accuracy: 0.9725\n",
      "Epoch [15/20], Loss: 0.0956, Accuracy: 0.9735\n",
      "Epoch [16/20], Loss: 0.0917, Accuracy: 0.9747\n",
      "Epoch [17/20], Loss: 0.0882, Accuracy: 0.9754\n",
      "Epoch [18/20], Loss: 0.0858, Accuracy: 0.9759\n",
      "Epoch [19/20], Loss: 0.0818, Accuracy: 0.9773\n",
      "Epoch [20/20], Loss: 0.0805, Accuracy: 0.9771\n",
      "Epoch [1/20], Loss: 0.1044, Accuracy: 0.9665\n",
      "Epoch [2/20], Loss: 0.0981, Accuracy: 0.9701\n",
      "Epoch [3/20], Loss: 0.0783, Accuracy: 0.9752\n",
      "Epoch [4/20], Loss: 0.0773, Accuracy: 0.9755\n",
      "Epoch [5/20], Loss: 0.0662, Accuracy: 0.9798\n",
      "Epoch [6/20], Loss: 0.0728, Accuracy: 0.9764\n",
      "Epoch [7/20], Loss: 0.0552, Accuracy: 0.9829\n",
      "Epoch [8/20], Loss: 0.0519, Accuracy: 0.9840\n",
      "Epoch [9/20], Loss: 0.0649, Accuracy: 0.9799\n",
      "Epoch [10/20], Loss: 0.0518, Accuracy: 0.9831\n",
      "Epoch [11/20], Loss: 0.0454, Accuracy: 0.9862\n",
      "Epoch [12/20], Loss: 0.0475, Accuracy: 0.9841\n",
      "Epoch [13/20], Loss: 0.0441, Accuracy: 0.9860\n",
      "Epoch [14/20], Loss: 0.0461, Accuracy: 0.9845\n",
      "Epoch [15/20], Loss: 0.0386, Accuracy: 0.9880\n",
      "Epoch [16/20], Loss: 0.0315, Accuracy: 0.9899\n",
      "Epoch [17/20], Loss: 0.0301, Accuracy: 0.9907\n",
      "Epoch [18/20], Loss: 0.0278, Accuracy: 0.9914\n",
      "Epoch [19/20], Loss: 0.0312, Accuracy: 0.9898\n",
      "Epoch [20/20], Loss: 0.0278, Accuracy: 0.9911\n",
      "Epoch [1/20], Loss: 0.0990, Accuracy: 0.9691\n",
      "Epoch [2/20], Loss: 0.0886, Accuracy: 0.9740\n",
      "Epoch [3/20], Loss: 0.0561, Accuracy: 0.9815\n",
      "Epoch [4/20], Loss: 0.0498, Accuracy: 0.9826\n",
      "Epoch [5/20], Loss: 0.0604, Accuracy: 0.9806\n",
      "Epoch [6/20], Loss: 0.0593, Accuracy: 0.9800\n",
      "Epoch [7/20], Loss: 0.0475, Accuracy: 0.9848\n",
      "Epoch [8/20], Loss: 0.0820, Accuracy: 0.9747\n",
      "Epoch [9/20], Loss: 0.0628, Accuracy: 0.9805\n",
      "Epoch [10/20], Loss: 0.0450, Accuracy: 0.9843\n",
      "Epoch [11/20], Loss: 0.0474, Accuracy: 0.9841\n",
      "Epoch [12/20], Loss: 0.0387, Accuracy: 0.9867\n",
      "Epoch [13/20], Loss: 0.0488, Accuracy: 0.9835\n",
      "Epoch [14/20], Loss: 0.0394, Accuracy: 0.9871\n",
      "Epoch [15/20], Loss: 0.0338, Accuracy: 0.9885\n",
      "Epoch [16/20], Loss: 0.0380, Accuracy: 0.9872\n",
      "Epoch [17/20], Loss: 0.0285, Accuracy: 0.9901\n",
      "Epoch [18/20], Loss: 0.0405, Accuracy: 0.9858\n",
      "Epoch [19/20], Loss: 0.0469, Accuracy: 0.9851\n",
      "Epoch [20/20], Loss: 0.0540, Accuracy: 0.9823\n",
      "Epoch [1/20], Loss: 1.2465, Accuracy: 0.7610\n",
      "Epoch [2/20], Loss: 0.6322, Accuracy: 0.8477\n",
      "Epoch [3/20], Loss: 0.4902, Accuracy: 0.8725\n",
      "Epoch [4/20], Loss: 0.4278, Accuracy: 0.8845\n",
      "Epoch [5/20], Loss: 0.3924, Accuracy: 0.8922\n",
      "Epoch [6/20], Loss: 0.3699, Accuracy: 0.8965\n",
      "Epoch [7/20], Loss: 0.3549, Accuracy: 0.8990\n",
      "Epoch [8/20], Loss: 0.3406, Accuracy: 0.9030\n",
      "Epoch [9/20], Loss: 0.3301, Accuracy: 0.9067\n",
      "Epoch [10/20], Loss: 0.3228, Accuracy: 0.9096\n",
      "Epoch [11/20], Loss: 0.3166, Accuracy: 0.9101\n",
      "Epoch [12/20], Loss: 0.3093, Accuracy: 0.9124\n",
      "Epoch [13/20], Loss: 0.3051, Accuracy: 0.9130\n",
      "Epoch [14/20], Loss: 0.3004, Accuracy: 0.9145\n",
      "Epoch [15/20], Loss: 0.2951, Accuracy: 0.9163\n",
      "Epoch [16/20], Loss: 0.2911, Accuracy: 0.9177\n",
      "Epoch [17/20], Loss: 0.2877, Accuracy: 0.9185\n",
      "Epoch [18/20], Loss: 0.2843, Accuracy: 0.9190\n",
      "Epoch [19/20], Loss: 0.2805, Accuracy: 0.9204\n",
      "Epoch [20/20], Loss: 0.2771, Accuracy: 0.9215\n",
      "Epoch [1/20], Loss: 0.2639, Accuracy: 0.9244\n",
      "Epoch [2/20], Loss: 0.2375, Accuracy: 0.9318\n",
      "Epoch [3/20], Loss: 0.2085, Accuracy: 0.9418\n",
      "Epoch [4/20], Loss: 0.1866, Accuracy: 0.9473\n",
      "Epoch [5/20], Loss: 0.1769, Accuracy: 0.9495\n",
      "Epoch [6/20], Loss: 0.1671, Accuracy: 0.9522\n",
      "Epoch [7/20], Loss: 0.1531, Accuracy: 0.9561\n",
      "Epoch [8/20], Loss: 0.1448, Accuracy: 0.9587\n",
      "Epoch [9/20], Loss: 0.1369, Accuracy: 0.9610\n",
      "Epoch [10/20], Loss: 0.1303, Accuracy: 0.9628\n",
      "Epoch [11/20], Loss: 0.1242, Accuracy: 0.9642\n",
      "Epoch [12/20], Loss: 0.1209, Accuracy: 0.9658\n",
      "Epoch [13/20], Loss: 0.1135, Accuracy: 0.9677\n",
      "Epoch [14/20], Loss: 0.1095, Accuracy: 0.9688\n",
      "Epoch [15/20], Loss: 0.1054, Accuracy: 0.9699\n",
      "Epoch [16/20], Loss: 0.1004, Accuracy: 0.9709\n",
      "Epoch [17/20], Loss: 0.1001, Accuracy: 0.9710\n",
      "Epoch [18/20], Loss: 0.0941, Accuracy: 0.9728\n",
      "Epoch [19/20], Loss: 0.0914, Accuracy: 0.9740\n",
      "Epoch [20/20], Loss: 0.0951, Accuracy: 0.9715\n",
      "Epoch [1/20], Loss: 0.1059, Accuracy: 0.9667\n",
      "Epoch [2/20], Loss: 0.0893, Accuracy: 0.9726\n",
      "Epoch [3/20], Loss: 0.0977, Accuracy: 0.9706\n",
      "Epoch [4/20], Loss: 0.0754, Accuracy: 0.9773\n",
      "Epoch [5/20], Loss: 0.0716, Accuracy: 0.9771\n",
      "Epoch [6/20], Loss: 0.0579, Accuracy: 0.9829\n",
      "Epoch [7/20], Loss: 0.0895, Accuracy: 0.9711\n",
      "Epoch [8/20], Loss: 0.0615, Accuracy: 0.9801\n",
      "Epoch [9/20], Loss: 0.0574, Accuracy: 0.9817\n",
      "Epoch [10/20], Loss: 0.0493, Accuracy: 0.9846\n",
      "Epoch [11/20], Loss: 0.0438, Accuracy: 0.9863\n",
      "Epoch [12/20], Loss: 0.0439, Accuracy: 0.9867\n",
      "Epoch [13/20], Loss: 0.0559, Accuracy: 0.9816\n",
      "Epoch [14/20], Loss: 0.0515, Accuracy: 0.9832\n",
      "Epoch [15/20], Loss: 0.0421, Accuracy: 0.9867\n",
      "Epoch [16/20], Loss: 0.0425, Accuracy: 0.9864\n",
      "Epoch [17/20], Loss: 0.0350, Accuracy: 0.9888\n",
      "Epoch [18/20], Loss: 0.0359, Accuracy: 0.9880\n",
      "Epoch [19/20], Loss: 0.0323, Accuracy: 0.9899\n",
      "Epoch [20/20], Loss: 0.0337, Accuracy: 0.9892\n",
      "Epoch [1/20], Loss: 0.0670, Accuracy: 0.9780\n",
      "Epoch [2/20], Loss: 0.0681, Accuracy: 0.9769\n",
      "Epoch [3/20], Loss: 0.0683, Accuracy: 0.9774\n",
      "Epoch [4/20], Loss: 0.0650, Accuracy: 0.9787\n",
      "Epoch [5/20], Loss: 0.0525, Accuracy: 0.9822\n",
      "Epoch [6/20], Loss: 0.0468, Accuracy: 0.9846\n",
      "Epoch [7/20], Loss: 0.0574, Accuracy: 0.9799\n",
      "Epoch [8/20], Loss: 0.0441, Accuracy: 0.9857\n",
      "Epoch [9/20], Loss: 0.0610, Accuracy: 0.9796\n",
      "Epoch [10/20], Loss: 0.0370, Accuracy: 0.9870\n",
      "Epoch [11/20], Loss: 0.0382, Accuracy: 0.9867\n",
      "Epoch [12/20], Loss: 0.0400, Accuracy: 0.9865\n",
      "Epoch [13/20], Loss: 0.0701, Accuracy: 0.9765\n",
      "Epoch [14/20], Loss: 0.0492, Accuracy: 0.9842\n",
      "Epoch [15/20], Loss: 0.0329, Accuracy: 0.9891\n",
      "Epoch [16/20], Loss: 0.0673, Accuracy: 0.9785\n",
      "Epoch [17/20], Loss: 0.0476, Accuracy: 0.9833\n",
      "Epoch [18/20], Loss: 0.0329, Accuracy: 0.9885\n",
      "Epoch [19/20], Loss: 0.0385, Accuracy: 0.9871\n",
      "Epoch [20/20], Loss: 0.0289, Accuracy: 0.9901\n",
      "[[[0.8075, 0.86, 0.8814, 0.8922, 0.898, 0.9019, 0.9059, 0.9085, 0.911, 0.9139, 0.9162, 0.9178, 0.9198, 0.9209, 0.9228, 0.9241, 0.925, 0.9273, 0.9281, 0.929], [0.86, 0.8814, 0.8922, 0.898, 0.9019, 0.9059, 0.9085, 0.911, 0.9139, 0.9162, 0.9178, 0.9198, 0.9209, 0.9228, 0.9241, 0.925, 0.9273, 0.9281, 0.929, 0.9368], [0.8814, 0.8922, 0.898, 0.9019, 0.9059, 0.9085, 0.911, 0.9139, 0.9162, 0.9178, 0.9198, 0.9209, 0.9228, 0.9241, 0.925, 0.9273, 0.9281, 0.929, 0.9368, 0.943], [0.8922, 0.898, 0.9019, 0.9059, 0.9085, 0.911, 0.9139, 0.9162, 0.9178, 0.9198, 0.9209, 0.9228, 0.9241, 0.925, 0.9273, 0.9281, 0.929, 0.9368, 0.943, 0.9482]], [[0.8164, 0.8667, 0.8825, 0.8914, 0.8979, 0.9019, 0.9053, 0.9085, 0.9113, 0.9137, 0.9148, 0.9171, 0.9182, 0.9202, 0.922, 0.9238, 0.9246, 0.9256, 0.9273, 0.9282], [0.8667, 0.8825, 0.8914, 0.8979, 0.9019, 0.9053, 0.9085, 0.9113, 0.9137, 0.9148, 0.9171, 0.9182, 0.9202, 0.922, 0.9238, 0.9246, 0.9256, 0.9273, 0.9282, 0.9364], [0.8825, 0.8914, 0.8979, 0.9019, 0.9053, 0.9085, 0.9113, 0.9137, 0.9148, 0.9171, 0.9182, 0.9202, 0.922, 0.9238, 0.9246, 0.9256, 0.9273, 0.9282, 0.9364, 0.9438], [0.8914, 0.8979, 0.9019, 0.9053, 0.9085, 0.9113, 0.9137, 0.9148, 0.9171, 0.9182, 0.9202, 0.922, 0.9238, 0.9246, 0.9256, 0.9273, 0.9282, 0.9364, 0.9438, 0.9501]], [[0.8036, 0.8552, 0.8755, 0.885, 0.8924, 0.898, 0.902, 0.9055, 0.9082, 0.911, 0.9136, 0.9155, 0.9178, 0.9194, 0.921, 0.9226, 0.9242, 0.9253, 0.9265, 0.9277], [0.8552, 0.8755, 0.885, 0.8924, 0.898, 0.902, 0.9055, 0.9082, 0.911, 0.9136, 0.9155, 0.9178, 0.9194, 0.921, 0.9226, 0.9242, 0.9253, 0.9265, 0.9277, 0.9366], [0.8755, 0.885, 0.8924, 0.898, 0.902, 0.9055, 0.9082, 0.911, 0.9136, 0.9155, 0.9178, 0.9194, 0.921, 0.9226, 0.9242, 0.9253, 0.9265, 0.9277, 0.9366, 0.9426], [0.885, 0.8924, 0.898, 0.902, 0.9055, 0.9082, 0.911, 0.9136, 0.9155, 0.9178, 0.9194, 0.921, 0.9226, 0.9242, 0.9253, 0.9265, 0.9277, 0.9366, 0.9426, 0.9483]], [[0.5468, 0.6696, 0.7061, 0.7512, 0.7902, 0.8108, 0.8235, 0.8382, 0.8483, 0.855, 0.8626, 0.8673, 0.8723, 0.8759, 0.8793, 0.8824, 0.8846, 0.8873, 0.8894, 0.8908], [0.6696, 0.7061, 0.7512, 0.7902, 0.8108, 0.8235, 0.8382, 0.8483, 0.855, 0.8626, 0.8673, 0.8723, 0.8759, 0.8793, 0.8824, 0.8846, 0.8873, 0.8894, 0.8908, 0.9035], [0.7061, 0.7512, 0.7902, 0.8108, 0.8235, 0.8382, 0.8483, 0.855, 0.8626, 0.8673, 0.8723, 0.8759, 0.8793, 0.8824, 0.8846, 0.8873, 0.8894, 0.8908, 0.9035, 0.9116], [0.7512, 0.7902, 0.8108, 0.8235, 0.8382, 0.8483, 0.855, 0.8626, 0.8673, 0.8723, 0.8759, 0.8793, 0.8824, 0.8846, 0.8873, 0.8894, 0.8908, 0.9035, 0.9116, 0.918]], [[0.8228, 0.8681, 0.8823, 0.8912, 0.8958, 0.9002, 0.9024, 0.906, 0.9083, 0.9101, 0.9116, 0.9138, 0.9156, 0.9168, 0.9173, 0.919, 0.9198, 0.921, 0.9222, 0.9232], [0.8681, 0.8823, 0.8912, 0.8958, 0.9002, 0.9024, 0.906, 0.9083, 0.9101, 0.9116, 0.9138, 0.9156, 0.9168, 0.9173, 0.919, 0.9198, 0.921, 0.9222, 0.9232, 0.9293], [0.8823, 0.8912, 0.8958, 0.9002, 0.9024, 0.906, 0.9083, 0.9101, 0.9116, 0.9138, 0.9156, 0.9168, 0.9173, 0.919, 0.9198, 0.921, 0.9222, 0.9232, 0.9293, 0.9384], [0.8912, 0.8958, 0.9002, 0.9024, 0.906, 0.9083, 0.9101, 0.9116, 0.9138, 0.9156, 0.9168, 0.9173, 0.919, 0.9198, 0.921, 0.9222, 0.9232, 0.9293, 0.9384, 0.9451]], [[0.761, 0.8477, 0.8725, 0.8845, 0.8922, 0.8965, 0.899, 0.903, 0.9067, 0.9096, 0.9101, 0.9124, 0.913, 0.9146, 0.9164, 0.9177, 0.9185, 0.919, 0.9204, 0.9215], [0.8477, 0.8725, 0.8845, 0.8922, 0.8965, 0.899, 0.903, 0.9067, 0.9096, 0.9101, 0.9124, 0.913, 0.9146, 0.9164, 0.9177, 0.9185, 0.919, 0.9204, 0.9215, 0.9244], [0.8725, 0.8845, 0.8922, 0.8965, 0.899, 0.903, 0.9067, 0.9096, 0.9101, 0.9124, 0.913, 0.9146, 0.9164, 0.9177, 0.9185, 0.919, 0.9204, 0.9215, 0.9244, 0.9318], [0.8845, 0.8922, 0.8965, 0.899, 0.903, 0.9067, 0.9096, 0.9101, 0.9124, 0.913, 0.9146, 0.9164, 0.9177, 0.9185, 0.919, 0.9204, 0.9215, 0.9244, 0.9318, 0.9418]]]\n",
      "[[[3.579, 3.568, 3.573, 3.566, 3.571, 3.577, 3.57, 3.573, 3.568, 3.569, 3.572, 3.562, 3.568, 3.568, 3.565, 3.57, 3.587, 3.572, 3.56, 3.577], [3.568, 3.573, 3.566, 3.571, 3.577, 3.57, 3.573, 3.568, 3.569, 3.572, 3.562, 3.568, 3.568, 3.565, 3.57, 3.587, 3.572, 3.56, 3.577, 3.571], [3.573, 3.566, 3.571, 3.577, 3.57, 3.573, 3.568, 3.569, 3.572, 3.562, 3.568, 3.568, 3.565, 3.57, 3.587, 3.572, 3.56, 3.577, 3.571, 3.559], [3.566, 3.571, 3.577, 3.57, 3.573, 3.568, 3.569, 3.572, 3.562, 3.568, 3.568, 3.565, 3.57, 3.587, 3.572, 3.56, 3.577, 3.571, 3.559, 3.557]], [[3.557, 3.557, 3.559, 3.561, 3.555, 3.561, 3.566, 3.564, 3.566, 3.554, 3.564, 3.56, 3.541, 3.553, 3.566, 3.559, 3.561, 3.57, 3.565, 3.561], [3.557, 3.559, 3.561, 3.555, 3.561, 3.566, 3.564, 3.566, 3.554, 3.564, 3.56, 3.541, 3.553, 3.566, 3.559, 3.561, 3.57, 3.565, 3.561, 3.561], [3.559, 3.561, 3.555, 3.561, 3.566, 3.564, 3.566, 3.554, 3.564, 3.56, 3.541, 3.553, 3.566, 3.559, 3.561, 3.57, 3.565, 3.561, 3.561, 3.559], [3.561, 3.555, 3.561, 3.566, 3.564, 3.566, 3.554, 3.564, 3.56, 3.541, 3.553, 3.566, 3.559, 3.561, 3.57, 3.565, 3.561, 3.561, 3.559, 3.552]], [[3.607, 3.617, 3.609, 3.622, 3.613, 3.615, 3.615, 3.615, 3.618, 3.61, 3.619, 3.613, 3.622, 3.614, 3.615, 3.615, 3.606, 3.633, 3.621, 3.613], [3.617, 3.609, 3.622, 3.613, 3.615, 3.615, 3.615, 3.618, 3.61, 3.619, 3.613, 3.622, 3.614, 3.615, 3.615, 3.606, 3.633, 3.621, 3.613, 3.625], [3.609, 3.622, 3.613, 3.615, 3.615, 3.615, 3.618, 3.61, 3.619, 3.613, 3.622, 3.614, 3.615, 3.615, 3.606, 3.633, 3.621, 3.613, 3.625, 3.62], [3.622, 3.613, 3.615, 3.615, 3.615, 3.618, 3.61, 3.619, 3.613, 3.622, 3.614, 3.615, 3.615, 3.606, 3.633, 3.621, 3.613, 3.625, 3.62, 3.622]], [[3.593, 3.577, 3.56, 3.561, 3.569, 3.564, 3.564, 3.574, 3.559, 3.564, 3.576, 3.568, 3.562, 3.565, 3.56, 3.56, 3.561, 3.561, 3.564, 3.564], [3.577, 3.56, 3.561, 3.569, 3.564, 3.564, 3.574, 3.559, 3.564, 3.576, 3.568, 3.562, 3.565, 3.56, 3.56, 3.561, 3.561, 3.564, 3.564, 3.57], [3.56, 3.561, 3.569, 3.564, 3.564, 3.574, 3.559, 3.564, 3.576, 3.568, 3.562, 3.565, 3.56, 3.56, 3.561, 3.561, 3.564, 3.564, 3.57, 3.572], [3.561, 3.569, 3.564, 3.564, 3.574, 3.559, 3.564, 3.576, 3.568, 3.562, 3.565, 3.56, 3.56, 3.561, 3.561, 3.564, 3.564, 3.57, 3.572, 3.561]], [[3.585, 3.574, 3.576, 3.573, 3.581, 3.575, 3.58, 3.572, 3.572, 3.58, 3.575, 3.577, 3.58, 3.583, 3.57, 3.577, 3.58, 3.579, 3.587, 3.567], [3.574, 3.576, 3.573, 3.581, 3.575, 3.58, 3.572, 3.572, 3.58, 3.575, 3.577, 3.58, 3.583, 3.57, 3.577, 3.58, 3.579, 3.587, 3.567, 3.58], [3.576, 3.573, 3.581, 3.575, 3.58, 3.572, 3.572, 3.58, 3.575, 3.577, 3.58, 3.583, 3.57, 3.577, 3.58, 3.579, 3.587, 3.567, 3.58, 3.59], [3.573, 3.581, 3.575, 3.58, 3.572, 3.572, 3.58, 3.575, 3.577, 3.58, 3.583, 3.57, 3.577, 3.58, 3.579, 3.587, 3.567, 3.58, 3.59, 3.579]], [[3.573, 3.581, 3.583, 3.573, 3.57, 3.578, 3.565, 3.576, 3.574, 3.579, 3.569, 3.571, 3.567, 3.581, 3.572, 3.571, 3.573, 3.571, 3.57, 3.566], [3.581, 3.583, 3.573, 3.57, 3.578, 3.565, 3.576, 3.574, 3.579, 3.569, 3.571, 3.567, 3.581, 3.572, 3.571, 3.573, 3.571, 3.57, 3.566, 3.575], [3.583, 3.573, 3.57, 3.578, 3.565, 3.576, 3.574, 3.579, 3.569, 3.571, 3.567, 3.581, 3.572, 3.571, 3.573, 3.571, 3.57, 3.566, 3.575, 3.577], [3.573, 3.57, 3.578, 3.565, 3.576, 3.574, 3.579, 3.569, 3.571, 3.567, 3.581, 3.572, 3.571, 3.573, 3.571, 3.57, 3.566, 3.575, 3.577, 3.583]]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "epochNum = 20\n",
    "#massive array\n",
    "#outer layer is by loss func, inners are all by learning rate\n",
    "results = []\n",
    "#by lossFunc\n",
    "byLossFunc = [] \n",
    "#long list of em\n",
    "byLR = []\n",
    "listTimes = []\n",
    "\n",
    "lossFuncs = [F.relu, F.leaky_relu, F.tanh, F.sigmoid, F.elu, F.softplus]\n",
    "\n",
    "trainingTimes = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for b in range (0, len(lossFuncs)):\n",
    "    byLossFunc.append([]) \n",
    "    trainingTimes.append([])\n",
    "\n",
    "dataset = MNIST(root = 'data/', download = True, transform = ToTensor())\n",
    "\n",
    "def splitIndices(n, valPct): #split data set into validation and training set, n is number of images, valPct is number you want to be validation set\n",
    "    nVal = int(valPct * n) #multiplying to find # of images to make validation\n",
    "    idxs = np.random.permutation(n) #creates a random permutation of n images from 0 to n-1 in the list of images\n",
    "    return idxs[nVal:], idxs[:nVal] #picks the first nVal indices to be used for validation set and returns trianing images and validation images split up and shuffled\n",
    "\n",
    "trainIndices, valIndices, = splitIndices(60000, 0.20)\n",
    "\n",
    "batchSize = 100\n",
    "\n",
    "trainSampler = SubsetRandomSampler(trainIndices)\n",
    "trainDL = DataLoader(dataset, batchSize, trainSampler)\n",
    "\n",
    "valSampler = SubsetRandomSampler(valIndices)\n",
    "valDL = DataLoader(dataset, batchSize, valSampler)\n",
    "\n",
    "def getDefaultDevice():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = getDefaultDevice()\n",
    "#each nn.Linear object we create, it creates 2 matrices of weights and biases to multiply inputs to get outputs we are going to create 2 so we have a hidden layer in between input and output\n",
    "for a in range (0, len(lossFuncs)):\n",
    "    byLR.clear()\n",
    "    listTimes.clear()\n",
    "        \n",
    "    class MnistModel(nn.Module):\n",
    "\n",
    "        \n",
    "        #create hidden layer and output layer\n",
    "        def __init__(self, inSize, hiddenSize1, hiddenSize2, hiddenSize3, hiddenSize4, hiddenSize5, outSize):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "            self.linear2 = nn.Linear(hiddenSize1, outSize).to(device)\n",
    "            # self.linear3 = nn.Linear(hiddenSize2, hiddenSize3).to(torch.device(device))\n",
    "            # self.linear4 = nn.Linear(hiddenSize3, hiddenSize4).to(torch.device(device))\n",
    "            # self.linear5 = nn.Linear(hiddenSize4, hiddenSize5).to(torch.device(device))\n",
    "            # self.linear6 = nn.Linear(hiddenSize5, outSize).to(torch.device(device))\n",
    "            \n",
    "\n",
    "        def forward(self, xb): #xb is batch of data\n",
    "            #flatten tensors\n",
    "            xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "            #immediate outputs\n",
    "            out = self.linear1(xb)\n",
    "            #apply activation function\n",
    "\n",
    "            if lossFuncs[a] == F.softmax:\n",
    "                 out = lossFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "            else:\n",
    "                out=lossFuncs[a](out)\n",
    "            #get predictions using output layer\n",
    "            out = self.linear2(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear3(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear4(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear5(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear6(out)\n",
    "           \n",
    "            return out\n",
    "        \n",
    "        \n",
    "    inputSize = 784\n",
    "    numClasses = 10\n",
    "\n",
    "    model = MnistModel(784, 256, 64, 32, 16, 12, 10) #dw about it, kthese are not the real values\n",
    "\n",
    "\n",
    "    for images, labels in trainDL:\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        #print('Loss:', loss.item())\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    def toDevice(data, device):\n",
    "        if isinstance(data, (list, tuple)): #if there are multiple tensors, move all to device\n",
    "            return [toDevice(x, device) for x in data] \n",
    "        return data.to(device, non_blocking = True)\n",
    "\n",
    "    # for images, labels in trainDL:\n",
    "    #     print(images.shape)\n",
    "    #     images = toDevice(images, device)\n",
    "    #     print(images.device)\n",
    "    #     break\n",
    "\n",
    "    #moves data to specific device in batches so not all at once\n",
    "    class DeviceDataLoader():\n",
    "        #wwraps a dataloader to move data to device\n",
    "        def __init__(self, DL, device):\n",
    "            self.DL = DL\n",
    "            self.device = device\n",
    "        #yields batch of data after moving it to device\n",
    "        def __iter__(self):\n",
    "            for b in self.DL:\n",
    "                yield toDevice(b, self.device)  \n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.DL)  # number of batches\n",
    "\n",
    "    # for xb, yb in valDL:\n",
    "    #     print('xb.device:', xb.device)\n",
    "    #     print('yb:', yb)\n",
    "    #     break\n",
    "\n",
    "    def lossBatch(model, lossFunc, xb, yb, opt = None, metric = None):\n",
    "        preds = model(xb) #gen predictions\n",
    "        loss = lossFunc(preds, yb)\n",
    "\n",
    "        if opt is not None:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        metricResult = None\n",
    "        if metric is not None:\n",
    "            metricResult = metric(preds, yb)\n",
    "        \n",
    "        return loss.item(), len(xb), metricResult\n",
    "\n",
    "\n",
    "    #calculates overall loss and a metric and also outputs total size of all batches together \n",
    "    def evaluate(model, lossFn, validDl, metric = None):\n",
    "    \n",
    "        # if len(byLR) == epochNum:\n",
    "        #     results.append(byLR)\n",
    "        #     byLR.clear()\n",
    "\n",
    "\n",
    "        with torch.no_grad(): #dont need to compute gradients with validation set, only for evaluation\n",
    "            results = [lossBatch(model, lossFn, xb, yb, metric = metric) \n",
    "                                        for xb, yb in validDl] #passes each batch through the model\n",
    "            \n",
    "            #seperate\n",
    "            losses, nums, metrics = zip(*results)\n",
    "\n",
    "            #total size is sum of all batch sizes\n",
    "            total = np.sum(nums)\n",
    "\n",
    "            avgLoss = np.sum(np.multiply(losses, nums)) / total #avg loss\n",
    "            avgMetric = None\n",
    "\n",
    "            if metric is not None:\n",
    "                #avg metric of assessment across all batches\n",
    "                avgMetric = np.sum(np.multiply(metrics, nums)) / total\n",
    "                    \n",
    "    \n",
    "        byLR.append(round(avgMetric, 4))\n",
    "\n",
    "\n",
    "        return avgLoss, total, avgMetric\n",
    "\n",
    "\n",
    "    def fit(epochs, lr, model, lossFN, trainDL, validDL, metric = None, optFN = None):\n",
    "        losses, metrics, = [], []\n",
    "\n",
    "        if optFN is None: optFN = torch.optim.SGD\n",
    "        opt = optFN(model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            time1 = time.perf_counter()\n",
    "            for xb, yb in trainDL:\n",
    "                lossBatch(model, lossFN, xb, yb, opt)\n",
    "\n",
    "            result  = evaluate(model, lossFN, validDL, metric)\n",
    "            valLoss, total, valMetric = result\n",
    "\n",
    "            losses.append(valLoss)\n",
    "            metrics.append(valMetric)\n",
    "\n",
    "            if metric is None:\n",
    "                    print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, epochs, valLoss))\n",
    "            \n",
    "            else:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                .format(epoch+1, epochs, valLoss, \"Accuracy\", valMetric))\n",
    "            time2 = time.perf_counter()\n",
    "\n",
    "            epochTime = time2-time1\n",
    "            listTimes.append(round(epochTime, 3))\n",
    "\n",
    "        return losses, metrics\n",
    "\n",
    "    def accuracy(outputs, labels):\n",
    "        _, preds = torch.max(outputs, dim = 1)\n",
    "        return torch.sum(preds == labels).item() / len(preds)\n",
    "\n",
    "    model = MnistModel(inputSize, hiddenSize1 = 32, hiddenSize2=0, hiddenSize3=0, hiddenSize4=0, hiddenSize5=0, outSize = numClasses)\n",
    "    toDevice(model, device)\n",
    "\n",
    "\n",
    "    # losses, metrics = fit(epochNum, 0.1, model, F.cross_entropy, trainDL, valDL, accuracy)\n",
    "\n",
    "    learningRates = [0.01,0.1,0.5,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def createArray():\n",
    "        for x in range(0, len(learningRates)):\n",
    "            #run this for every learning rate\n",
    "            fit(epochNum, learningRates[x], model, F.cross_entropy, trainDL, valDL, accuracy)\n",
    "        #loop through byLR\n",
    "\n",
    "        for y in range(0, len(learningRates)):\n",
    "            tempAcc = []\n",
    "            tempTimes = []\n",
    "            for z in range (0, epochNum):\n",
    "                tempAcc.append(byLR[y+z])\n",
    "                tempTimes.append(listTimes[y+z])\n",
    "            \n",
    "            byLossFunc[a].append(tempAcc)\n",
    "            trainingTimes[a].append(tempTimes)\n",
    "        \n",
    "\n",
    "    createArray()\n",
    "print(byLossFunc)\n",
    "print(trainingTimes)\n",
    "\n",
    "\n",
    "# print(\"lr=\"+lr)\n",
    "# print(\"epochs=\"+epochNum)\n",
    "\n",
    "\n",
    "#originally only had one hidden layer, but changed to two which slightly increased accuracy from 98.4 to 99 :) (2 hidden layers of 64, 32)\n",
    "#with 3 hidden layers (64, 32, 16) i got to 99.7 in 20 epochs\n",
    "\n",
    "#(act func is relu)with 4 hidden layers (128, 64, 32, 16) i got to 99.91 in 20 epochs, loss and accuracy jumped around when it flattened out within a percent or so even though learning rate was only 0.08\n",
    "# with act funcs of sigmoid, relu, leakyrelu and relu, accuracy dropped to 97.81 after 20 epochs\n",
    "\n",
    "#20 epochs with 32 64 128 32 got 99.91\n",
    "\n",
    "#20 epochs with 4 hidden layers, 512 1024 256 64 i got accuracy of 100% starting at 16 epochs, ended with a loss of 0.0004, it is important to note\n",
    "#that it took much longer to train (7 minutes ish)\n",
    "\n",
    "#same excpet config is 1024 2048 512 64 got accuray of 100% on the 16th epoch as well, ended with loss of 0.0003 took same training time\n",
    "\n",
    "#20 epochs hdiden layer sizes of 1024 1536 768 256 64 learning rate of 0.1 took a long time to train () because of extra hidden layer and last layer using sigmoid\n",
    "#anywhere i use sigmoid takes way longer to train after 17 epochs i got 99.94 pretty bad took like 15 - 20 minutes to run\n",
    "\n",
    "# 20 epoch same sizes but all relu got 100% on epoch 14 and loss of 0.0001 at epoch 18 and ended with 0.0001, however, it took 17 minutes to train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03b0a5ea72e569892ff75cdce4f0a43aa28d5543ecfacea9505a52dbab1ee89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
