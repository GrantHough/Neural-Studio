{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8867, Accuracy: 0.8148\n",
      "Epoch [2/10], Loss: 0.5409, Accuracy: 0.8661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 257\u001b[0m\n\u001b[1;32m    253\u001b[0m             byLossFunc[a]\u001b[39m.\u001b[39mappend(tempAcc)\n\u001b[1;32m    254\u001b[0m             trainingTimes[a]\u001b[39m.\u001b[39mappend(tempTimes)\n\u001b[0;32m--> 257\u001b[0m     createArray()\n\u001b[1;32m    258\u001b[0m \u001b[39mprint\u001b[39m(byLossFunc)\n\u001b[1;32m    259\u001b[0m \u001b[39mprint\u001b[39m(trainingTimes)\n",
      "Cell \u001b[0;32mIn[1], line 243\u001b[0m, in \u001b[0;36mcreateArray\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreateArray\u001b[39m():\n\u001b[1;32m    241\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(learningRates)):\n\u001b[1;32m    242\u001b[0m         \u001b[39m#run this for every learning rate\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m         fit(epochNum, learningRates[x], model, F\u001b[39m.\u001b[39;49mcross_entropy, trainDL, valDL, accuracy)\n\u001b[1;32m    244\u001b[0m     \u001b[39m#loop through byLR\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(learningRates)):\n",
      "Cell \u001b[0;32mIn[1], line 203\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, lr, model, lossFN, trainDL, validDL, metric, optFN)\u001b[0m\n\u001b[1;32m    201\u001b[0m time1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m    202\u001b[0m \u001b[39mfor\u001b[39;00m xb, yb \u001b[39min\u001b[39;00m trainDL:\n\u001b[0;32m--> 203\u001b[0m     lossBatch(model, lossFN, xb, yb, opt)\n\u001b[1;32m    205\u001b[0m result  \u001b[39m=\u001b[39m evaluate(model, lossFN, validDL, metric)\n\u001b[1;32m    206\u001b[0m valLoss, total, valMetric \u001b[39m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[1], line 153\u001b[0m, in \u001b[0;36mlossBatch\u001b[0;34m(model, lossFunc, xb, yb, opt, metric)\u001b[0m\n\u001b[1;32m    151\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    152\u001b[0m     opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 153\u001b[0m     opt\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m    155\u001b[0m metricResult \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m metric \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/wwdc2023/lib/python3.11/site-packages/torch/optim/optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mif\u001b[39;00m foreach:\n\u001b[1;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[0;32m--> 456\u001b[0m \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_zero_grad_profile_name):\n\u001b[1;32m    457\u001b[0m     \u001b[39mfor\u001b[39;49;00m group \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_groups:\n\u001b[1;32m    458\u001b[0m         \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m group[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/wwdc2023/lib/python3.11/site-packages/torch/autograd/profiler.py:507\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    506\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 507\u001b[0m         torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_exit\u001b[39m.\u001b[39;49m_RecordFunction(record)\n\u001b[1;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_exit(record)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "epochNum = 10\n",
    "#massive array\n",
    "#outer layer is by loss func, inners are all by learning rate\n",
    "results = []\n",
    "#by lossFunc\n",
    "byLossFunc = [] \n",
    "#long list of em\n",
    "byLR = []\n",
    "listTimes = []\n",
    "\n",
    "lossFuncs = [F.relu, F.leaky_relu, F.tanh, F.sigmoid, F.elu, F.softplus]\n",
    "\n",
    "trainingTimes = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for b in range (0, len(lossFuncs)):\n",
    "    byLossFunc.append([]) \n",
    "    trainingTimes.append([])\n",
    "\n",
    "dataset = MNIST(root = 'data/', download = True, transform = ToTensor())\n",
    "\n",
    "def splitIndices(n, valPct): #split data set into validation and training set, n is number of images, valPct is number you want to be validation set\n",
    "    nVal = int(valPct * n) #multiplying to find # of images to make validation\n",
    "    idxs = np.random.permutation(n) #creates a random permutation of n images from 0 to n-1 in the list of images\n",
    "    return idxs[nVal:], idxs[:nVal] #picks the first nVal indices to be used for validation set and returns trianing images and validation images split up and shuffled\n",
    "\n",
    "trainIndices, valIndices, = splitIndices(60000, 0.20)\n",
    "\n",
    "batchSize = 100\n",
    "\n",
    "trainSampler = SubsetRandomSampler(trainIndices)\n",
    "trainDL = DataLoader(dataset, batchSize, trainSampler)\n",
    "\n",
    "valSampler = SubsetRandomSampler(valIndices)\n",
    "valDL = DataLoader(dataset, batchSize, valSampler)\n",
    "\n",
    "def getDefaultDevice():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = getDefaultDevice()\n",
    "#each nn.Linear object we create, it creates 2 matrices of weights and biases to multiply inputs to get outputs we are going to create 2 so we have a hidden layer in between input and output\n",
    "for a in range (0, len(lossFuncs)):\n",
    "    byLR.clear()\n",
    "    listTimes.clear()\n",
    "        \n",
    "    class MnistModel(nn.Module):\n",
    "\n",
    "        \n",
    "        #create hidden layer and output layer\n",
    "        def __init__(self, inSize, hiddenSize1, hiddenSize2, hiddenSize3, hiddenSize4, hiddenSize5, outSize):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "            self.linear2 = nn.Linear(hiddenSize1, outSize).to(device)\n",
    "            # self.linear3 = nn.Linear(hiddenSize2, hiddenSize3).to(torch.device(device))\n",
    "            # self.linear4 = nn.Linear(hiddenSize3, hiddenSize4).to(torch.device(device))\n",
    "            # self.linear5 = nn.Linear(hiddenSize4, hiddenSize5).to(torch.device(device))\n",
    "            # self.linear6 = nn.Linear(hiddenSize5, outSize).to(torch.device(device))\n",
    "            \n",
    "\n",
    "        def forward(self, xb): #xb is batch of data\n",
    "            #flatten tensors\n",
    "            xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "            #immediate outputs\n",
    "            out = self.linear1(xb)\n",
    "            #apply activation function\n",
    "\n",
    "            if lossFuncs[a] == F.softmax:\n",
    "                 out = lossFuncs[a](out, -2) #stock relu just turns negatives into 0\n",
    "            else:\n",
    "                out=lossFuncs[a](out)\n",
    "            #get predictions using output layer\n",
    "            out = self.linear2(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear3(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear4(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear5(out)\n",
    "            # out = F.relu(out)\n",
    "            # out = self.linear6(out)\n",
    "           \n",
    "            return out\n",
    "        \n",
    "        \n",
    "    inputSize = 784\n",
    "    numClasses = 10\n",
    "\n",
    "    model = MnistModel(784, 256, 64, 32, 16, 12, 10) #dw about it, kthese are not the real values\n",
    "\n",
    "\n",
    "    for images, labels in trainDL:\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        #print('Loss:', loss.item())\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    def toDevice(data, device):\n",
    "        if isinstance(data, (list, tuple)): #if there are multiple tensors, move all to device\n",
    "            return [toDevice(x, device) for x in data] \n",
    "        return data.to(device, non_blocking = True)\n",
    "\n",
    "    # for images, labels in trainDL:\n",
    "    #     print(images.shape)\n",
    "    #     images = toDevice(images, device)\n",
    "    #     print(images.device)\n",
    "    #     break\n",
    "\n",
    "    #moves data to specific device in batches so not all at once\n",
    "    class DeviceDataLoader():\n",
    "        #wwraps a dataloader to move data to device\n",
    "        def __init__(self, DL, device):\n",
    "            self.DL = DL\n",
    "            self.device = device\n",
    "        #yields batch of data after moving it to device\n",
    "        def __iter__(self):\n",
    "            for b in self.DL:\n",
    "                yield toDevice(b, self.device)  \n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.DL)  # number of batches\n",
    "\n",
    "    # for xb, yb in valDL:\n",
    "    #     print('xb.device:', xb.device)\n",
    "    #     print('yb:', yb)\n",
    "    #     break\n",
    "\n",
    "    def lossBatch(model, lossFunc, xb, yb, opt = None, metric = None):\n",
    "        preds = model(xb) #gen predictions\n",
    "        loss = lossFunc(preds, yb)\n",
    "\n",
    "        if opt is not None:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        metricResult = None\n",
    "        if metric is not None:\n",
    "            metricResult = metric(preds, yb)\n",
    "        \n",
    "        return loss.item(), len(xb), metricResult\n",
    "\n",
    "\n",
    "    #calculates overall loss and a metric and also outputs total size of all batches together \n",
    "    def evaluate(model, lossFn, validDl, metric = None):\n",
    "    \n",
    "        # if len(byLR) == epochNum:\n",
    "        #     results.append(byLR)\n",
    "        #     byLR.clear()\n",
    "\n",
    "\n",
    "        with torch.no_grad(): #dont need to compute gradients with validation set, only for evaluation\n",
    "            results = [lossBatch(model, lossFn, xb, yb, metric = metric) \n",
    "                                        for xb, yb in validDl] #passes each batch through the model\n",
    "            \n",
    "            #seperate\n",
    "            losses, nums, metrics = zip(*results)\n",
    "\n",
    "            #total size is sum of all batch sizes\n",
    "            total = np.sum(nums)\n",
    "\n",
    "            avgLoss = np.sum(np.multiply(losses, nums)) / total #avg loss\n",
    "            avgMetric = None\n",
    "\n",
    "            if metric is not None:\n",
    "                #avg metric of assessment across all batches\n",
    "                avgMetric = np.sum(np.multiply(metrics, nums)) / total\n",
    "                    \n",
    "    \n",
    "        byLR.append(round(avgMetric, 4))\n",
    "\n",
    "\n",
    "        return avgLoss, total, avgMetric\n",
    "\n",
    "\n",
    "    def fit(epochs, lr, model, lossFN, trainDL, validDL, metric = None, optFN = None):\n",
    "        losses, metrics, = [], []\n",
    "\n",
    "        if optFN is None: optFN = torch.optim.SGD\n",
    "        opt = optFN(model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            time1 = time.perf_counter()\n",
    "            for xb, yb in trainDL:\n",
    "                lossBatch(model, lossFN, xb, yb, opt)\n",
    "\n",
    "            result  = evaluate(model, lossFN, validDL, metric)\n",
    "            valLoss, total, valMetric = result\n",
    "\n",
    "            losses.append(valLoss)\n",
    "            metrics.append(valMetric)\n",
    "\n",
    "            if metric is None:\n",
    "                    print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, epochs, valLoss))\n",
    "            \n",
    "            else:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                .format(epoch+1, epochs, valLoss, \"Accuracy\", valMetric))\n",
    "            time2 = time.perf_counter()\n",
    "\n",
    "            epochTime = time2-time1\n",
    "            listTimes.append(round(epochTime, 3))\n",
    "\n",
    "        return losses, metrics\n",
    "\n",
    "    def accuracy(outputs, labels):\n",
    "        _, preds = torch.max(outputs, dim = 1)\n",
    "        return torch.sum(preds == labels).item() / len(preds)\n",
    "\n",
    "    model = MnistModel(inputSize, hiddenSize1 = 32, hiddenSize2=0, hiddenSize3=0, hiddenSize4=0, hiddenSize5=0, outSize = numClasses)\n",
    "    toDevice(model, device)\n",
    "\n",
    "\n",
    "    # losses, metrics = fit(epochNum, 0.1, model, F.cross_entropy, trainDL, valDL, accuracy)\n",
    "\n",
    "    learningRates = [0.01,0.05,0.1,0.5,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def createArray():\n",
    "        for x in range(0, len(learningRates)):\n",
    "            #run this for every learning rate\n",
    "            fit(epochNum, learningRates[x], model, F.cross_entropy, trainDL, valDL, accuracy)\n",
    "        #loop through byLR\n",
    "\n",
    "        for y in range(0, len(learningRates)):\n",
    "            tempAcc = []\n",
    "            tempTimes = []\n",
    "            for z in range (0, epochNum):\n",
    "                tempAcc.append(byLR[y+z])\n",
    "                tempTimes.append(listTimes[y+z])\n",
    "            \n",
    "            byLossFunc[a].append(tempAcc)\n",
    "            trainingTimes[a].append(tempTimes)\n",
    "        \n",
    "\n",
    "    createArray()\n",
    "print(byLossFunc)\n",
    "print(trainingTimes)\n",
    "\n",
    "\n",
    "# print(\"lr=\"+lr)\n",
    "# print(\"epochs=\"+epochNum)\n",
    "\n",
    "\n",
    "#originally only had one hidden layer, but changed to two which slightly increased accuracy from 98.4 to 99 :) (2 hidden layers of 64, 32)\n",
    "#with 3 hidden layers (64, 32, 16) i got to 99.7 in 20 epochs\n",
    "\n",
    "#(act func is relu)with 4 hidden layers (128, 64, 32, 16) i got to 99.91 in 20 epochs, loss and accuracy jumped around when it flattened out within a percent or so even though learning rate was only 0.08\n",
    "# with act funcs of sigmoid, relu, leakyrelu and relu, accuracy dropped to 97.81 after 20 epochs\n",
    "\n",
    "#20 epochs with 32 64 128 32 got 99.91\n",
    "\n",
    "#20 epochs with 4 hidden layers, 512 1024 256 64 i got accuracy of 100% starting at 16 epochs, ended with a loss of 0.0004, it is important to note\n",
    "#that it took much longer to train (7 minutes ish)\n",
    "\n",
    "#same excpet config is 1024 2048 512 64 got accuray of 100% on the 16th epoch as well, ended with loss of 0.0003 took same training time\n",
    "\n",
    "#20 epochs hdiden layer sizes of 1024 1536 768 256 64 learning rate of 0.1 took a long time to train () because of extra hidden layer and last layer using sigmoid\n",
    "#anywhere i use sigmoid takes way longer to train after 17 epochs i got 99.94 pretty bad took like 15 - 20 minutes to run\n",
    "\n",
    "# 20 epoch same sizes but all relu got 100% on epoch 14 and loss of 0.0001 at epoch 18 and ended with 0.0001, however, it took 17 minutes to train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03b0a5ea72e569892ff75cdce4f0a43aa28d5543ecfacea9505a52dbab1ee89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
