{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.2955, Accuracy: 0.0993\n",
      "Epoch [2/20], Loss: 2.2645, Accuracy: 0.2072\n",
      "Epoch [3/20], Loss: 1.8366, Accuracy: 0.5265\n",
      "Epoch [4/20], Loss: 0.6846, Accuracy: 0.7927\n",
      "Epoch [5/20], Loss: 0.5121, Accuracy: 0.8487\n",
      "Epoch [6/20], Loss: 0.4464, Accuracy: 0.8709\n",
      "Epoch [7/20], Loss: 0.4058, Accuracy: 0.8830\n",
      "Epoch [8/20], Loss: 0.3699, Accuracy: 0.8938\n",
      "Epoch [9/20], Loss: 0.3345, Accuracy: 0.9046\n",
      "Epoch [10/20], Loss: 0.3031, Accuracy: 0.9133\n",
      "Epoch [11/20], Loss: 0.2793, Accuracy: 0.9196\n",
      "Epoch [12/20], Loss: 0.2552, Accuracy: 0.9272\n",
      "Epoch [13/20], Loss: 0.2362, Accuracy: 0.9315\n",
      "Epoch [14/20], Loss: 0.2162, Accuracy: 0.9374\n",
      "Epoch [15/20], Loss: 0.1935, Accuracy: 0.9443\n",
      "Epoch [16/20], Loss: 0.1839, Accuracy: 0.9465\n",
      "Epoch [17/20], Loss: 0.1703, Accuracy: 0.9514\n",
      "Epoch [18/20], Loss: 0.1604, Accuracy: 0.9541\n",
      "Epoch [19/20], Loss: 0.1496, Accuracy: 0.9566\n",
      "Epoch [20/20], Loss: 0.1440, Accuracy: 0.9586\n",
      "Epoch [1/20], Loss: 0.1359, Accuracy: 0.9592\n",
      "Epoch [2/20], Loss: 0.0967, Accuracy: 0.9721\n",
      "Epoch [3/20], Loss: 0.0654, Accuracy: 0.9818\n",
      "Epoch [4/20], Loss: 0.0566, Accuracy: 0.9837\n",
      "Epoch [5/20], Loss: 0.0614, Accuracy: 0.9812\n",
      "Epoch [6/20], Loss: 0.0374, Accuracy: 0.9892\n",
      "Epoch [7/20], Loss: 0.0446, Accuracy: 0.9860\n",
      "Epoch [8/20], Loss: 0.0279, Accuracy: 0.9924\n",
      "Epoch [9/20], Loss: 0.0267, Accuracy: 0.9921\n",
      "Epoch [10/20], Loss: 0.0205, Accuracy: 0.9941\n",
      "Epoch [11/20], Loss: 0.0154, Accuracy: 0.9955\n",
      "Epoch [12/20], Loss: 0.0135, Accuracy: 0.9963\n",
      "Epoch [13/20], Loss: 0.0151, Accuracy: 0.9953\n",
      "Epoch [14/20], Loss: 0.0097, Accuracy: 0.9974\n",
      "Epoch [15/20], Loss: 0.0088, Accuracy: 0.9977\n",
      "Epoch [16/20], Loss: 0.0045, Accuracy: 0.9991\n",
      "Epoch [17/20], Loss: 0.0037, Accuracy: 0.9993\n",
      "Epoch [18/20], Loss: 0.0049, Accuracy: 0.9989\n",
      "Epoch [19/20], Loss: 0.0034, Accuracy: 0.9992\n",
      "Epoch [20/20], Loss: 0.0047, Accuracy: 0.9987\n",
      "Epoch [1/20], Loss: 2.3018, Accuracy: 0.1124\n",
      "Epoch [2/20], Loss: 2.3017, Accuracy: 0.1124\n",
      "Epoch [3/20], Loss: 2.3026, Accuracy: 0.1124\n",
      "Epoch [4/20], Loss: 2.3014, Accuracy: 0.1124\n",
      "Epoch [5/20], Loss: 2.3018, Accuracy: 0.1124\n",
      "Epoch [6/20], Loss: 2.3016, Accuracy: 0.1124\n",
      "Epoch [7/20], Loss: 2.3016, Accuracy: 0.1022\n",
      "Epoch [8/20], Loss: 2.3027, Accuracy: 0.1124\n",
      "Epoch [9/20], Loss: 2.3021, Accuracy: 0.1044\n",
      "Epoch [10/20], Loss: 2.3021, Accuracy: 0.1124\n",
      "Epoch [11/20], Loss: 2.3027, Accuracy: 0.0992\n",
      "Epoch [12/20], Loss: 2.3022, Accuracy: 0.1124\n",
      "Epoch [13/20], Loss: 2.3025, Accuracy: 0.0993\n",
      "Epoch [14/20], Loss: 2.3022, Accuracy: 0.1044\n",
      "Epoch [15/20], Loss: 2.3015, Accuracy: 0.1124\n",
      "Epoch [16/20], Loss: 2.3017, Accuracy: 0.1124\n",
      "Epoch [17/20], Loss: 2.3023, Accuracy: 0.1124\n",
      "Epoch [18/20], Loss: 2.3028, Accuracy: 0.0987\n",
      "Epoch [19/20], Loss: 2.3022, Accuracy: 0.1022\n",
      "Epoch [20/20], Loss: 2.3016, Accuracy: 0.1124\n",
      "Epoch [1/20], Loss: 2.3044, Accuracy: 0.0992\n",
      "Epoch [2/20], Loss: 2.3033, Accuracy: 0.1124\n",
      "Epoch [3/20], Loss: 2.3027, Accuracy: 0.1044\n",
      "Epoch [4/20], Loss: 2.3060, Accuracy: 0.1124\n",
      "Epoch [5/20], Loss: 2.3042, Accuracy: 0.0987\n",
      "Epoch [6/20], Loss: 2.3047, Accuracy: 0.1044\n",
      "Epoch [7/20], Loss: 2.3035, Accuracy: 0.1124\n",
      "Epoch [8/20], Loss: 2.3031, Accuracy: 0.0986\n",
      "Epoch [9/20], Loss: 2.3047, Accuracy: 0.1044\n",
      "Epoch [10/20], Loss: 2.3022, Accuracy: 0.1124\n",
      "Epoch [11/20], Loss: 2.3039, Accuracy: 0.0987\n",
      "Epoch [12/20], Loss: 2.3029, Accuracy: 0.1124\n",
      "Epoch [13/20], Loss: 2.3023, Accuracy: 0.1044\n",
      "Epoch [14/20], Loss: 2.3024, Accuracy: 0.1124\n",
      "Epoch [15/20], Loss: 2.3020, Accuracy: 0.1124\n",
      "Epoch [16/20], Loss: 2.3040, Accuracy: 0.1124\n",
      "Epoch [17/20], Loss: 2.3028, Accuracy: 0.1124\n",
      "Epoch [18/20], Loss: 2.3043, Accuracy: 0.1022\n",
      "Epoch [19/20], Loss: 2.3047, Accuracy: 0.0992\n",
      "Epoch [20/20], Loss: 2.3038, Accuracy: 0.1044\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "Epoch [1/20], Loss: 2.2894, Accuracy: 0.2554\n",
      "Epoch [2/20], Loss: 2.2126, Accuracy: 0.4177\n",
      "Epoch [3/20], Loss: 1.3277, Accuracy: 0.5916\n",
      "Epoch [4/20], Loss: 0.6248, Accuracy: 0.8050\n",
      "Epoch [5/20], Loss: 0.4914, Accuracy: 0.8552\n",
      "Epoch [6/20], Loss: 0.4329, Accuracy: 0.8744\n",
      "Epoch [7/20], Loss: 0.3924, Accuracy: 0.8872\n",
      "Epoch [8/20], Loss: 0.3379, Accuracy: 0.9023\n",
      "Epoch [9/20], Loss: 0.2990, Accuracy: 0.9143\n",
      "Epoch [10/20], Loss: 0.2720, Accuracy: 0.9222\n",
      "Epoch [11/20], Loss: 0.2573, Accuracy: 0.9256\n",
      "Epoch [12/20], Loss: 0.2295, Accuracy: 0.9337\n",
      "Epoch [13/20], Loss: 0.2128, Accuracy: 0.9384\n",
      "Epoch [14/20], Loss: 0.1923, Accuracy: 0.9452\n",
      "Epoch [15/20], Loss: 0.1784, Accuracy: 0.9498\n",
      "Epoch [16/20], Loss: 0.1632, Accuracy: 0.9530\n",
      "Epoch [17/20], Loss: 0.1531, Accuracy: 0.9559\n",
      "Epoch [18/20], Loss: 0.1419, Accuracy: 0.9595\n",
      "Epoch [19/20], Loss: 0.1360, Accuracy: 0.9602\n",
      "Epoch [20/20], Loss: 0.1326, Accuracy: 0.9614\n",
      "Epoch [1/20], Loss: 0.1243, Accuracy: 0.9651\n",
      "Epoch [2/20], Loss: 0.0962, Accuracy: 0.9713\n",
      "Epoch [3/20], Loss: 0.0711, Accuracy: 0.9783\n",
      "Epoch [4/20], Loss: 0.0573, Accuracy: 0.9828\n",
      "Epoch [5/20], Loss: 0.0529, Accuracy: 0.9839\n",
      "Epoch [6/20], Loss: 0.0492, Accuracy: 0.9847\n",
      "Epoch [7/20], Loss: 0.0520, Accuracy: 0.9834\n",
      "Epoch [8/20], Loss: 0.0310, Accuracy: 0.9905\n",
      "Epoch [9/20], Loss: 0.0218, Accuracy: 0.9936\n",
      "Epoch [10/20], Loss: 0.0326, Accuracy: 0.9891\n",
      "Epoch [11/20], Loss: 0.0180, Accuracy: 0.9944\n",
      "Epoch [12/20], Loss: 0.0165, Accuracy: 0.9954\n",
      "Epoch [13/20], Loss: 0.0137, Accuracy: 0.9960\n",
      "Epoch [14/20], Loss: 0.0085, Accuracy: 0.9979\n",
      "Epoch [15/20], Loss: 0.0275, Accuracy: 0.9899\n",
      "Epoch [16/20], Loss: 0.0169, Accuracy: 0.9946\n",
      "Epoch [17/20], Loss: 0.0081, Accuracy: 0.9978\n",
      "Epoch [18/20], Loss: 0.0088, Accuracy: 0.9975\n",
      "Epoch [19/20], Loss: 0.0089, Accuracy: 0.9974\n",
      "Epoch [20/20], Loss: 0.0064, Accuracy: 0.9981\n",
      "Epoch [1/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [2/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [3/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [4/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [5/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [6/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [7/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [8/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [9/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [10/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [11/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [12/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [13/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [14/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [15/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [16/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [17/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [18/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [19/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [20/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [1/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [2/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [3/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [4/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [5/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [6/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [7/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [8/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [9/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [10/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [11/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [12/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [13/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [14/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [15/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [16/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [17/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [18/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [19/20], Loss: nan, Accuracy: 0.0987\n",
      "Epoch [20/20], Loss: nan, Accuracy: 0.0987\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "Epoch [1/20], Loss: 1.6379, Accuracy: 0.5539\n",
      "Epoch [2/20], Loss: 0.8726, Accuracy: 0.7662\n",
      "Epoch [3/20], Loss: 0.5819, Accuracy: 0.8450\n",
      "Epoch [4/20], Loss: 0.4609, Accuracy: 0.8756\n",
      "Epoch [5/20], Loss: 0.3993, Accuracy: 0.8892\n",
      "Epoch [6/20], Loss: 0.3631, Accuracy: 0.8982\n",
      "Epoch [7/20], Loss: 0.3387, Accuracy: 0.9042\n",
      "Epoch [8/20], Loss: 0.3179, Accuracy: 0.9095\n",
      "Epoch [9/20], Loss: 0.3018, Accuracy: 0.9141\n",
      "Epoch [10/20], Loss: 0.2879, Accuracy: 0.9178\n",
      "Epoch [11/20], Loss: 0.2764, Accuracy: 0.9214\n",
      "Epoch [12/20], Loss: 0.2628, Accuracy: 0.9252\n",
      "Epoch [13/20], Loss: 0.2504, Accuracy: 0.9283\n",
      "Epoch [14/20], Loss: 0.2396, Accuracy: 0.9318\n",
      "Epoch [15/20], Loss: 0.2307, Accuracy: 0.9337\n",
      "Epoch [16/20], Loss: 0.2172, Accuracy: 0.9381\n",
      "Epoch [17/20], Loss: 0.2076, Accuracy: 0.9406\n",
      "Epoch [18/20], Loss: 0.1982, Accuracy: 0.9434\n",
      "Epoch [19/20], Loss: 0.1908, Accuracy: 0.9456\n",
      "Epoch [20/20], Loss: 0.1812, Accuracy: 0.9486\n",
      "Epoch [1/20], Loss: 0.1495, Accuracy: 0.9565\n",
      "Epoch [2/20], Loss: 0.1503, Accuracy: 0.9550\n",
      "Epoch [3/20], Loss: 0.0971, Accuracy: 0.9716\n",
      "Epoch [4/20], Loss: 0.0894, Accuracy: 0.9737\n",
      "Epoch [5/20], Loss: 0.0735, Accuracy: 0.9791\n",
      "Epoch [6/20], Loss: 0.0665, Accuracy: 0.9805\n",
      "Epoch [7/20], Loss: 0.0551, Accuracy: 0.9844\n",
      "Epoch [8/20], Loss: 0.0601, Accuracy: 0.9813\n",
      "Epoch [9/20], Loss: 0.0427, Accuracy: 0.9879\n",
      "Epoch [10/20], Loss: 0.0452, Accuracy: 0.9860\n",
      "Epoch [11/20], Loss: 0.0369, Accuracy: 0.9897\n",
      "Epoch [12/20], Loss: 0.0352, Accuracy: 0.9899\n",
      "Epoch [13/20], Loss: 0.0295, Accuracy: 0.9917\n",
      "Epoch [14/20], Loss: 0.0275, Accuracy: 0.9921\n",
      "Epoch [15/20], Loss: 0.0222, Accuracy: 0.9943\n",
      "Epoch [16/20], Loss: 0.0212, Accuracy: 0.9944\n",
      "Epoch [17/20], Loss: 0.0213, Accuracy: 0.9943\n",
      "Epoch [18/20], Loss: 0.0219, Accuracy: 0.9935\n",
      "Epoch [19/20], Loss: 0.0190, Accuracy: 0.9946\n",
      "Epoch [20/20], Loss: 0.0133, Accuracy: 0.9970\n",
      "Epoch [1/20], Loss: 0.1042, Accuracy: 0.9676\n",
      "Epoch [2/20], Loss: 0.0644, Accuracy: 0.9790\n",
      "Epoch [3/20], Loss: 0.0528, Accuracy: 0.9836\n",
      "Epoch [4/20], Loss: 0.0326, Accuracy: 0.9899\n",
      "Epoch [5/20], Loss: 0.0349, Accuracy: 0.9887\n",
      "Epoch [6/20], Loss: 0.0292, Accuracy: 0.9908\n",
      "Epoch [7/20], Loss: 0.0183, Accuracy: 0.9943\n",
      "Epoch [8/20], Loss: 0.0247, Accuracy: 0.9920\n",
      "Epoch [9/20], Loss: 0.0196, Accuracy: 0.9939\n",
      "Epoch [10/20], Loss: 0.0157, Accuracy: 0.9952\n",
      "Epoch [11/20], Loss: 0.0149, Accuracy: 0.9954\n",
      "Epoch [12/20], Loss: 0.0219, Accuracy: 0.9939\n",
      "Epoch [13/20], Loss: 0.0091, Accuracy: 0.9972\n",
      "Epoch [14/20], Loss: 0.0084, Accuracy: 0.9977\n",
      "Epoch [15/20], Loss: 0.0087, Accuracy: 0.9973\n",
      "Epoch [16/20], Loss: 0.0071, Accuracy: 0.9979\n",
      "Epoch [17/20], Loss: 0.0058, Accuracy: 0.9983\n",
      "Epoch [18/20], Loss: 0.0072, Accuracy: 0.9977\n",
      "Epoch [19/20], Loss: 0.0056, Accuracy: 0.9985\n",
      "Epoch [20/20], Loss: 0.0104, Accuracy: 0.9966\n",
      "Epoch [1/20], Loss: 0.0522, Accuracy: 0.9829\n",
      "Epoch [2/20], Loss: 0.0426, Accuracy: 0.9859\n",
      "Epoch [3/20], Loss: 0.0330, Accuracy: 0.9897\n",
      "Epoch [4/20], Loss: 0.0332, Accuracy: 0.9891\n",
      "Epoch [5/20], Loss: 0.0335, Accuracy: 0.9892\n",
      "Epoch [6/20], Loss: 0.0192, Accuracy: 0.9939\n",
      "Epoch [7/20], Loss: 0.0183, Accuracy: 0.9943\n",
      "Epoch [8/20], Loss: 0.0212, Accuracy: 0.9935\n",
      "Epoch [9/20], Loss: 0.0184, Accuracy: 0.9942\n",
      "Epoch [10/20], Loss: 0.0384, Accuracy: 0.9878\n",
      "Epoch [11/20], Loss: 0.0202, Accuracy: 0.9936\n",
      "Epoch [12/20], Loss: 0.0275, Accuracy: 0.9916\n",
      "Epoch [13/20], Loss: 0.0273, Accuracy: 0.9912\n",
      "Epoch [14/20], Loss: 0.0162, Accuracy: 0.9949\n",
      "Epoch [15/20], Loss: 0.0154, Accuracy: 0.9951\n",
      "Epoch [16/20], Loss: 0.0150, Accuracy: 0.9950\n",
      "Epoch [17/20], Loss: 0.0131, Accuracy: 0.9958\n",
      "Epoch [18/20], Loss: 0.0111, Accuracy: 0.9964\n",
      "Epoch [19/20], Loss: 0.0134, Accuracy: 0.9959\n",
      "Epoch [20/20], Loss: 0.0141, Accuracy: 0.9956\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "Epoch [1/20], Loss: 2.3014, Accuracy: 0.1124\n",
      "Epoch [2/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [3/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [4/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [5/20], Loss: 2.3014, Accuracy: 0.1124\n",
      "Epoch [6/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [7/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [8/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [9/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [10/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [11/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [12/20], Loss: 2.3014, Accuracy: 0.1124\n",
      "Epoch [13/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [14/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [15/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [16/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [17/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [18/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [19/20], Loss: 2.3013, Accuracy: 0.1124\n",
      "Epoch [20/20], Loss: 2.3014, Accuracy: 0.1124\n",
      "Epoch [1/20], Loss: 2.3028, Accuracy: 0.1124\n",
      "Epoch [2/20], Loss: 2.3027, Accuracy: 0.1022\n",
      "Epoch [3/20], Loss: 2.3026, Accuracy: 0.1124\n",
      "Epoch [4/20], Loss: 2.3025, Accuracy: 0.0975\n",
      "Epoch [5/20], Loss: 2.3016, Accuracy: 0.1022\n",
      "Epoch [6/20], Loss: 2.3031, Accuracy: 0.1124\n",
      "Epoch [7/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [8/20], Loss: 2.3018, Accuracy: 0.1124\n",
      "Epoch [9/20], Loss: 2.3017, Accuracy: 0.0974\n",
      "Epoch [10/20], Loss: 2.3015, Accuracy: 0.1124\n",
      "Epoch [11/20], Loss: 2.3012, Accuracy: 0.1124\n",
      "Epoch [12/20], Loss: 2.3009, Accuracy: 0.1124\n",
      "Epoch [13/20], Loss: 2.3004, Accuracy: 0.1124\n",
      "Epoch [14/20], Loss: 2.3006, Accuracy: 0.1124\n",
      "Epoch [15/20], Loss: 2.2993, Accuracy: 0.1124\n",
      "Epoch [16/20], Loss: 2.2973, Accuracy: 0.2046\n",
      "Epoch [17/20], Loss: 2.2823, Accuracy: 0.2123\n",
      "Epoch [18/20], Loss: 1.9062, Accuracy: 0.2797\n",
      "Epoch [19/20], Loss: 1.6994, Accuracy: 0.3167\n",
      "Epoch [20/20], Loss: 1.5832, Accuracy: 0.3785\n",
      "Epoch [1/20], Loss: 1.3245, Accuracy: 0.4949\n",
      "Epoch [2/20], Loss: 1.0060, Accuracy: 0.6303\n",
      "Epoch [3/20], Loss: 0.6338, Accuracy: 0.7963\n",
      "Epoch [4/20], Loss: 0.4616, Accuracy: 0.8696\n",
      "Epoch [5/20], Loss: 0.4111, Accuracy: 0.8810\n",
      "Epoch [6/20], Loss: 0.2915, Accuracy: 0.9207\n",
      "Epoch [7/20], Loss: 0.2229, Accuracy: 0.9382\n",
      "Epoch [8/20], Loss: 0.2026, Accuracy: 0.9432\n",
      "Epoch [9/20], Loss: 0.1689, Accuracy: 0.9536\n",
      "Epoch [10/20], Loss: 0.1615, Accuracy: 0.9548\n",
      "Epoch [11/20], Loss: 0.1387, Accuracy: 0.9609\n",
      "Epoch [12/20], Loss: 0.1204, Accuracy: 0.9672\n",
      "Epoch [13/20], Loss: 0.1182, Accuracy: 0.9665\n",
      "Epoch [14/20], Loss: 0.1031, Accuracy: 0.9714\n",
      "Epoch [15/20], Loss: 0.0957, Accuracy: 0.9727\n",
      "Epoch [16/20], Loss: 0.0854, Accuracy: 0.9758\n",
      "Epoch [17/20], Loss: 0.0931, Accuracy: 0.9725\n",
      "Epoch [18/20], Loss: 0.0702, Accuracy: 0.9811\n",
      "Epoch [19/20], Loss: 0.0754, Accuracy: 0.9775\n",
      "Epoch [20/20], Loss: 0.0572, Accuracy: 0.9846\n",
      "Epoch [1/20], Loss: 0.0713, Accuracy: 0.9790\n",
      "Epoch [2/20], Loss: 0.0584, Accuracy: 0.9835\n",
      "Epoch [3/20], Loss: 0.0553, Accuracy: 0.9846\n",
      "Epoch [4/20], Loss: 0.0443, Accuracy: 0.9874\n",
      "Epoch [5/20], Loss: 0.0734, Accuracy: 0.9765\n",
      "Epoch [6/20], Loss: 0.0365, Accuracy: 0.9902\n",
      "Epoch [7/20], Loss: 0.0365, Accuracy: 0.9896\n",
      "Epoch [8/20], Loss: 0.0304, Accuracy: 0.9912\n",
      "Epoch [9/20], Loss: 0.0330, Accuracy: 0.9897\n",
      "Epoch [10/20], Loss: 0.0306, Accuracy: 0.9916\n",
      "Epoch [11/20], Loss: 0.0255, Accuracy: 0.9929\n",
      "Epoch [12/20], Loss: 0.0421, Accuracy: 0.9868\n",
      "Epoch [13/20], Loss: 0.0291, Accuracy: 0.9909\n",
      "Epoch [14/20], Loss: 0.0185, Accuracy: 0.9949\n",
      "Epoch [15/20], Loss: 0.0232, Accuracy: 0.9930\n",
      "Epoch [16/20], Loss: 0.0165, Accuracy: 0.9953\n",
      "Epoch [17/20], Loss: 0.0129, Accuracy: 0.9968\n",
      "Epoch [18/20], Loss: 0.0099, Accuracy: 0.9979\n",
      "Epoch [19/20], Loss: 0.0134, Accuracy: 0.9965\n",
      "Epoch [20/20], Loss: 0.0272, Accuracy: 0.9918\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "model saving\n",
      "[[[0.0993, 0.2072, 0.5265, 0.7927, 0.8487, 0.8709, 0.883, 0.8938, 0.9046, 0.9133, 0.9196, 0.9272, 0.9315, 0.9374, 0.9443, 0.9465, 0.9514, 0.9541, 0.9566, 0.9586], [0.9592, 0.9721, 0.9818, 0.9837, 0.9812, 0.9892, 0.986, 0.9924, 0.9921, 0.9941, 0.9955, 0.9964, 0.9953, 0.9974, 0.9977, 0.9991, 0.9993, 0.9989, 0.9992, 0.9987], [0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1022, 0.1124, 0.1044, 0.1124, 0.0992, 0.1124, 0.0993, 0.1044, 0.1124, 0.1124, 0.1124, 0.0987, 0.1022, 0.1124], [0.0992, 0.1124, 0.1044, 0.1124, 0.0987, 0.1044, 0.1124, 0.0986, 0.1044, 0.1124, 0.0987, 0.1124, 0.1044, 0.1124, 0.1124, 0.1124, 0.1124, 0.1022, 0.0992, 0.1044]], [[0.2554, 0.4177, 0.5916, 0.805, 0.8552, 0.8744, 0.8872, 0.9023, 0.9143, 0.9222, 0.9256, 0.9337, 0.9384, 0.9452, 0.9498, 0.953, 0.9559, 0.9594, 0.9602, 0.9614], [0.9652, 0.9713, 0.9783, 0.9828, 0.9839, 0.9847, 0.9834, 0.9905, 0.9936, 0.9891, 0.9944, 0.9954, 0.996, 0.9979, 0.9899, 0.9946, 0.9978, 0.9975, 0.9974, 0.9981], [0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987], [0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987, 0.0987]], [[0.5539, 0.7662, 0.845, 0.8756, 0.8892, 0.8982, 0.9042, 0.9095, 0.9141, 0.9178, 0.9214, 0.9252, 0.9283, 0.9318, 0.9337, 0.938, 0.9406, 0.9434, 0.9456, 0.9486], [0.9565, 0.955, 0.9716, 0.9737, 0.9791, 0.9804, 0.9844, 0.9813, 0.988, 0.986, 0.9897, 0.9899, 0.9916, 0.9921, 0.9944, 0.9944, 0.9943, 0.9934, 0.9946, 0.997], [0.9676, 0.979, 0.9836, 0.9899, 0.9887, 0.9908, 0.9943, 0.992, 0.9939, 0.9952, 0.9954, 0.9939, 0.9972, 0.9977, 0.9973, 0.9979, 0.9983, 0.9977, 0.9985, 0.9966], [0.9828, 0.986, 0.9897, 0.9892, 0.9892, 0.9939, 0.9943, 0.9934, 0.9942, 0.9878, 0.9936, 0.9916, 0.9912, 0.9948, 0.9951, 0.995, 0.9958, 0.9964, 0.9959, 0.9956]], [[0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124], [0.1124, 0.1022, 0.1124, 0.0975, 0.1022, 0.1124, 0.1124, 0.1124, 0.0974, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.1124, 0.2046, 0.2123, 0.2797, 0.3167, 0.3785], [0.4949, 0.6303, 0.7963, 0.8696, 0.881, 0.9208, 0.9382, 0.9432, 0.9536, 0.9548, 0.9609, 0.9672, 0.9665, 0.9714, 0.9726, 0.9758, 0.9725, 0.9811, 0.9775, 0.9846], [0.979, 0.9835, 0.9846, 0.9874, 0.9765, 0.9902, 0.9896, 0.9912, 0.9896, 0.9916, 0.9929, 0.9868, 0.9909, 0.9949, 0.993, 0.9953, 0.9968, 0.9979, 0.9965, 0.9918]]]\n",
      "[[[3.872, 3.769, 3.738, 3.681, 3.655, 3.657, 3.637, 3.644, 3.657, 3.684, 3.701, 3.655, 3.667, 3.68, 3.649, 3.66, 3.651, 3.677, 3.678, 3.693], [3.672, 3.675, 3.647, 3.66, 3.66, 3.677, 3.663, 3.651, 3.671, 3.68, 3.656, 3.662, 3.65, 3.662, 3.667, 3.659, 3.642, 3.667, 3.65, 3.648], [3.642, 3.639, 3.649, 3.638, 3.638, 3.642, 3.675, 3.665, 3.647, 3.637, 3.663, 3.661, 3.639, 3.64, 3.667, 3.654, 3.628, 3.638, 3.654, 3.645], [3.643, 3.655, 3.688, 3.647, 3.632, 3.672, 3.67, 3.628, 3.642, 3.658, 3.66, 3.631, 3.642, 3.678, 3.638, 3.675, 3.65, 3.655, 3.653, 3.651]], [[3.641, 3.647, 3.636, 3.68, 3.66, 3.681, 3.673, 3.829, 3.816, 3.816, 3.806, 3.825, 3.813, 3.829, 3.795, 3.835, 3.84, 3.803, 3.818, 3.77], [3.789, 3.706, 3.736, 3.72, 3.702, 3.717, 3.717, 3.721, 3.719, 3.694, 3.725, 3.728, 3.706, 3.71, 3.697, 3.706, 3.717, 3.687, 3.774, 3.793], [3.711, 3.689, 3.768, 3.719, 3.693, 3.713, 3.71, 3.723, 3.69, 3.67, 3.698, 3.675, 3.729, 3.687, 3.698, 3.673, 3.691, 3.668, 3.699, 3.7], [3.704, 3.69, 3.686, 3.691, 3.718, 3.699, 3.698, 3.686, 3.742, 3.707, 3.663, 3.683, 3.7, 3.677, 3.688, 3.7, 3.696, 3.688, 3.742, 3.693]], [[4.007, 3.976, 3.986, 3.992, 4.014, 4.002, 4.016, 3.98, 4.099, 4.085, 4.259, 4.106, 4.05, 4.036, 4.046, 4.02, 4.007, 3.959, 3.978, 3.927], [4.083, 4.332, 4.0, 3.993, 3.986, 3.96, 3.985, 3.965, 3.993, 3.964, 3.982, 3.954, 3.981, 3.961, 3.958, 4.005, 3.977, 3.967, 3.985, 4.0], [3.967, 3.974, 3.98, 3.993, 3.978, 3.966, 3.973, 3.999, 3.985, 3.983, 3.998, 3.987, 4.098, 4.307, 4.259, 4.13, 4.2, 4.106, 4.01, 3.985], [3.981, 4.001, 3.978, 3.97, 3.994, 3.981, 3.972, 3.987, 3.986, 3.993, 3.973, 3.965, 3.964, 3.998, 3.966, 3.97, 4.019, 3.997, 4.005, 4.072]], [[3.866, 3.834, 3.833, 3.866, 3.854, 3.847, 3.826, 3.816, 3.836, 3.847, 3.845, 3.828, 3.845, 3.834, 3.845, 3.841, 3.872, 3.827, 3.837, 3.869], [3.865, 3.836, 3.858, 3.859, 3.908, 3.84, 3.845, 3.842, 3.869, 3.851, 3.821, 3.853, 3.877, 3.832, 3.855, 3.869, 3.873, 3.878, 3.919, 3.901], [3.878, 3.89, 3.88, 3.889, 3.887, 3.873, 3.862, 3.868, 3.871, 3.849, 3.849, 3.872, 3.879, 3.84, 3.886, 3.848, 3.849, 3.84, 3.835, 3.843], [3.85, 3.872, 3.856, 3.866, 3.84, 3.847, 3.857, 3.883, 3.855, 3.855, 3.842, 3.862, 3.85, 3.844, 3.844, 3.87, 3.859, 3.84, 3.86, 3.848]]]\n",
      "[[73.705, 73.219, 72.961, 73.068], [75.21300000000001, 74.46400000000001, 74.004, 73.95100000000001], [80.545, 80.031, 80.878, 79.772], [76.86800000000001, 77.25099999999999, 77.288, 77.1]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "epochNum = 20\n",
    "#massive array\n",
    "#outer layer is by loss func, inners are all by learning rate\n",
    "results = []\n",
    "#by activationFunc\n",
    "byActivationFunc = [] \n",
    "#long list of em\n",
    "byLR = []\n",
    "listTimes = []\n",
    " \n",
    "activationFuncs = [F.relu, F.leaky_relu, F.tanh, F.sigmoid]\n",
    "\n",
    "hiddenLayers = [1, 2, 3, 4]\n",
    "\n",
    "#actually in arrays\n",
    "trainingTimes = []\n",
    "\n",
    "#in arrays and summed up\n",
    "summedTimes = []\n",
    "\n",
    "#by KBs\n",
    "calculatedModelSizes = [97.084, 200.844, 240.395, 322.487]\n",
    "\n",
    "#actual model sizes in KBs\n",
    "\n",
    "actualModelSizes = [103, 212, 254, 341]\n",
    "\n",
    "\n",
    "amountHiddenLayers = 4\n",
    "\n",
    "\n",
    "for b in range (0, len(activationFuncs)):\n",
    "    byActivationFunc.append([]) \n",
    "    trainingTimes.append([])\n",
    "    summedTimes.append([])\n",
    "\n",
    "dataset = MNIST(root = 'data/', download = True, transform = ToTensor())\n",
    "\n",
    "def splitIndices(n, valPct): #split data set into validation and training set, n is number of images, valPct is number you want to be validation set\n",
    "    nVal = int(valPct * n) #multiplying to find # of images to make validation\n",
    "    idxs = np.random.permutation(n) #creates a random permutation of n images from 0 to n-1 in the list of images\n",
    "    return idxs[nVal:], idxs[:nVal] #picks the first nVal indices to be used for validation set and returns trianing images and validation images split up and shuffled\n",
    "\n",
    "trainIndices, valIndices, = splitIndices(60000, 0.20)\n",
    "\n",
    "batchSize = 100\n",
    "\n",
    "trainSampler = SubsetRandomSampler(trainIndices)\n",
    "trainDL = DataLoader(dataset, batchSize, trainSampler)\n",
    "\n",
    "valSampler = SubsetRandomSampler(valIndices)\n",
    "valDL = DataLoader(dataset, batchSize, valSampler)\n",
    "\n",
    "def getDefaultDevice():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('cpu')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = getDefaultDevice()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#each nn.Linear object we create, it creates 2 matrices of weights and biases to multiply inputs to get outputs we are going to create 2 so we have a hidden layer in between input and output\n",
    "for a in range (0, len(activationFuncs)):\n",
    "    byLR.clear()\n",
    "    listTimes.clear()\n",
    "        \n",
    "    class MnistModel(nn.Module):\n",
    "\n",
    "        \n",
    "        #create hidden layer and output layer\n",
    "        def __init__(self, inSize, hiddenSize1, hiddenSize2, hiddenSize3, hiddenSize4, hiddenSize5, outSize):\n",
    "            super().__init__()\n",
    "\n",
    "            if (amountHiddenLayers == 1):\n",
    "                    self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "                    self.linear2 = nn.Linear(hiddenSize1, outSize).to(device)\n",
    "            elif (amountHiddenLayers == 2):\n",
    "                self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "                self.linear2 = nn.Linear(hiddenSize1, hiddenSize2).to(device)\n",
    "                self.linear3 = nn.Linear(hiddenSize2, outSize).to(device)\n",
    "            elif (amountHiddenLayers == 3):\n",
    "                self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "                self.linear2 = nn.Linear(hiddenSize1, hiddenSize2).to(device)\n",
    "                self.linear3 = nn.Linear(hiddenSize2, hiddenSize3).to(device)\n",
    "                self.linear4 = nn.Linear(hiddenSize3, outSize).to(device)\n",
    "            elif (amountHiddenLayers == 4):\n",
    "                self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "                self.linear2 = nn.Linear(hiddenSize1, hiddenSize2).to(device)\n",
    "                self.linear3 = nn.Linear(hiddenSize2, hiddenSize3).to(device)\n",
    "                self.linear4 = nn.Linear(hiddenSize3, hiddenSize4).to(device)\n",
    "                self.linear5 = nn.Linear(hiddenSize4, outSize).to(device)\n",
    "            else:\n",
    "                self.linear1 = nn.Linear(inSize, hiddenSize1).to(device)\n",
    "                self.linear2 = nn.Linear(hiddenSize1, outSize).to(device)\n",
    "        \n",
    "\n",
    "        def forward(self, xb): #xb is batch of data\n",
    "            if (amountHiddenLayers == 1):\n",
    "                        \n",
    "                #flatten tensors\n",
    "                xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "                #immediate outputs\n",
    "                out = self.linear1(xb)\n",
    "                #apply activation function\n",
    "\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                #get predictions using output layer\n",
    "                out = self.linear2(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear3(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear4(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear5(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear6(out)\n",
    "            \n",
    "                return out\n",
    "            \n",
    "            \n",
    "            elif (amountHiddenLayers == 2):\n",
    "                        \n",
    "                #flatten tensors\n",
    "                xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "                #immediate outputs\n",
    "                out = self.linear1(xb)\n",
    "                #apply activation function\n",
    "\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                #get predictions using output layer\n",
    "                out = self.linear2(out)\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                out = self.linear3(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear4(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear5(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear6(out)\n",
    "            \n",
    "                return out\n",
    "            \n",
    "            \n",
    "            elif (amountHiddenLayers == 3):\n",
    "                        #flatten tensors\n",
    "                xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "                #immediate outputs\n",
    "                out = self.linear1(xb)\n",
    "                #apply activation function\n",
    "\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                #get predictions using output layer\n",
    "                out = self.linear2(out)\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                out = self.linear3(out)\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                out = self.linear4(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear5(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear6(out)\n",
    "            \n",
    "                return out\n",
    "            \n",
    "                            \n",
    "            elif (amountHiddenLayers == 4):\n",
    "                        #flatten tensors\n",
    "                xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "                #immediate outputs\n",
    "                out = self.linear1(xb)\n",
    "                #apply activation function\n",
    "\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                #get predictions using output layer\n",
    "                out = self.linear2(out)\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                out = self.linear3(out)\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                out = self.linear4(out)\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                out = self.linear5(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear6(out)\n",
    "            \n",
    "                return out\n",
    "            \n",
    "            else:\n",
    "                #flatten tensors\n",
    "                xb = xb.view(xb.size(0), -1) #xb.size(0) retains batch size input, -1 makes pytorch calculate so the model is general and can be used with images that dont have 784 pixels\n",
    "                #immediate outputs\n",
    "                out = self.linear1(xb)\n",
    "                #apply activation function\n",
    "\n",
    "                if activationFuncs[a] == F.softmax:\n",
    "                    out = activationFuncs[a](out, 1) #stock relu just turns negatives into 0\n",
    "                else:\n",
    "                    out=activationFuncs[a](out)\n",
    "                #get predictions using output layer\n",
    "                out = self.linear2(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear3(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear4(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear5(out)\n",
    "                # out = F.relu(out)\n",
    "                # out = self.linear6(out)\n",
    "            \n",
    "                return out\n",
    "        \n",
    "    inputSize = 784\n",
    "    numClasses = 10\n",
    "\n",
    "    model = MnistModel(784, 64, 32, 16, 16, 12, 10) #dw about it, kthese are not the real values\n",
    "\n",
    "\n",
    "    for images, labels in trainDL:\n",
    "        outputs = model(images)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        #print('Loss:', loss.item())\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "    def toDevice(data, device):\n",
    "        if isinstance(data, (list, tuple)): #if there are multiple tensors, move all to device\n",
    "            return [toDevice(x, device) for x in data] \n",
    "        return data.to(device, non_blocking = True)\n",
    "\n",
    "    # for images, labels in trainDL:\n",
    "    #     print(images.shape)\n",
    "    #     images = toDevice(images, device)\n",
    "    #     print(images.device)\n",
    "    #     break\n",
    "\n",
    "    #moves data to specific device in batches so not all at once\n",
    "    class DeviceDataLoader():\n",
    "        #wwraps a dataloader to move data to device\n",
    "        def __init__(self, DL, device):\n",
    "            self.DL = DL\n",
    "            self.device = device\n",
    "        #yields batch of data after moving it to device\n",
    "        def __iter__(self):\n",
    "            for b in self.DL:\n",
    "                yield toDevice(b, self.device)  \n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.DL)  # number of batches\n",
    "\n",
    "    # for xb, yb in valDL:\n",
    "    #     print('xb.device:', xb.device)\n",
    "    #     print('yb:', yb)\n",
    "    #     break\n",
    "\n",
    "    def lossBatch(model, lossFunc, xb, yb, opt = None, metric = None):\n",
    "        preds = model(xb) #gen predictions\n",
    "        loss = lossFunc(preds, yb)\n",
    "\n",
    "        if opt is not None:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        metricResult = None\n",
    "        if metric is not None:\n",
    "            metricResult = metric(preds, yb)\n",
    "        \n",
    "        return loss.item(), len(xb), metricResult\n",
    "\n",
    "\n",
    "    #calculates overall loss and a metric and also outputs total size of all batches together \n",
    "    def evaluate(model, lossFn, validDl, metric = None):\n",
    "    \n",
    "        # if len(byLR) == epochNum:\n",
    "        #     results.append(byLR)\n",
    "        #     byLR.clear()\n",
    "\n",
    "\n",
    "        with torch.no_grad(): #dont need to compute gradients with validation set, only for evaluation\n",
    "            results = [lossBatch(model, lossFn, xb, yb, metric = metric) \n",
    "                                        for xb, yb in validDl] #passes each batch through the model\n",
    "            \n",
    "            #seperate\n",
    "            losses, nums, metrics = zip(*results)\n",
    "\n",
    "            #total size is sum of all batch sizes\n",
    "            total = np.sum(nums)\n",
    "\n",
    "            avgLoss = np.sum(np.multiply(losses, nums)) / total #avg loss\n",
    "            avgMetric = None\n",
    "\n",
    "            if metric is not None:\n",
    "                #avg metric of assessment across all batches\n",
    "                avgMetric = np.sum(np.multiply(metrics, nums)) / total\n",
    "                    \n",
    "    \n",
    "        byLR.append(round(avgMetric, 4))\n",
    "\n",
    "\n",
    "        return avgLoss, total, avgMetric\n",
    "\n",
    "\n",
    "    def fit(epochs, lr, model, lossFN, trainDL, validDL, metric = None, optFN = None):\n",
    "        losses, metrics, = [], []\n",
    "\n",
    "        if optFN is None: optFN = torch.optim.SGD\n",
    "        opt = optFN(model.parameters(), lr = lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            time1 = time.perf_counter()\n",
    "            for xb, yb in trainDL:\n",
    "                lossBatch(model, lossFN, xb, yb, opt)\n",
    "\n",
    "            result  = evaluate(model, lossFN, validDL, metric)\n",
    "            valLoss, total, valMetric = result\n",
    "\n",
    "            losses.append(valLoss)\n",
    "            metrics.append(valMetric)\n",
    "\n",
    "            if metric is None:\n",
    "                    print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, epochs, valLoss))\n",
    "            \n",
    "            else:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                .format(epoch+1, epochs, valLoss, \"Accuracy\", valMetric))\n",
    "            time2 = time.perf_counter()\n",
    "\n",
    "            epochTime = time2-time1\n",
    "            listTimes.append(round(epochTime, 3))\n",
    "\n",
    "        return losses, metrics\n",
    "\n",
    "    def accuracy(outputs, labels):\n",
    "        _, preds = torch.max(outputs, dim = 1)\n",
    "        return torch.sum(preds == labels).item() / len(preds)\n",
    "\n",
    "    if (amountHiddenLayers == 1):\n",
    "        model = MnistModel(inputSize, hiddenSize1 = 32, hiddenSize2=128, hiddenSize3=32, hiddenSize4=16, hiddenSize5=0, outSize = numClasses)\n",
    "    elif (amountHiddenLayers==2):\n",
    "\n",
    "        model = MnistModel(inputSize, hiddenSize1 = 64, hiddenSize2=32, hiddenSize3=0, hiddenSize4=0, hiddenSize5=0, outSize = numClasses)\n",
    "    elif(amountHiddenLayers==3):\n",
    "        model = MnistModel(inputSize, hiddenSize1 = 64, hiddenSize2=128, hiddenSize3=32, hiddenSize4=0, hiddenSize5=0, outSize = numClasses)\n",
    "    elif(amountHiddenLayers==4):\n",
    "        model = MnistModel(inputSize, hiddenSize1 = 128, hiddenSize2=512, hiddenSize3=128, hiddenSize4=32, hiddenSize5=0, outSize = numClasses)\n",
    "    toDevice(model, device)\n",
    "\n",
    "\n",
    "    # losses, metrics = fit(epochNum, 0.1, model, F.cross_entropy, trainDL, valDL, accuracy)\n",
    "\n",
    "    learningRates = [0.01,0.1,0.5,1]\n",
    "\n",
    "    def createArray():\n",
    "        for x in range(0, len(learningRates)):\n",
    "            #run this for every learning rate\n",
    "            fit(epochNum, learningRates[x], model, F.cross_entropy, trainDL, valDL, accuracy)\n",
    "          \n",
    "            # torch.save(model.state_dict(), 'model2.pt')\n",
    "\n",
    "        for y in range(0, len(learningRates)):\n",
    "            tempAcc = []\n",
    "            tempTimes = []\n",
    "            totalTime = 0\n",
    "            for z in range (0, epochNum):\n",
    "                tempAcc.append(byLR[epochNum*y+z])\n",
    "                tempTimes.append(listTimes[epochNum*y+z])\n",
    "                totalTime+=listTimes[epochNum*y+z]\n",
    "            \n",
    "            byActivationFunc[a].append(tempAcc)\n",
    "            trainingTimes[a].append(tempTimes)\n",
    "            summedTimes[a].append(totalTime)\n",
    "     \n",
    "            \n",
    "            #generate model size    \n",
    "            paramSize = 0\n",
    "            for param in model.parameters():\n",
    "                paramSize += param.nelement() * param.element_size()\n",
    "            sizeMB = (paramSize) / 1024**2\n",
    "            sizeKB = sizeMB*1000\n",
    "            # print('model size: {:.3f}KB'.format(sizeKB))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    createArray()\n",
    "print(byActivationFunc)\n",
    "print(trainingTimes)\n",
    "print(summedTimes)\n",
    "\n",
    "\n",
    "# print(\"lr=\"+lr)\n",
    "# print(\"epochs=\"+epochNum)\n",
    "\n",
    "\n",
    "#originally only had one hidden layer, but changed to two which slightly increased accuracy from 98.4 to 99 :) (2 hidden layers of 64, 32)\n",
    "#with 3 hidden layers (64, 32, 16) i got to 99.7 in 20 epochs\n",
    "\n",
    "#(act func is relu)with 4 hidden layers (128, 64, 32, 16) i got to 99.91 in 20 epochs, loss and accuracy jumped around when it flattened out within a percent or so even though learning rate was only 0.08\n",
    "# with act funcs of sigmoid, relu, leakyrelu and relu, accuracy dropped to 97.81 after 20 epochs\n",
    "\n",
    "#20 epochs with 32 64 128 32 got 99.91\n",
    "\n",
    "\n",
    "\n",
    "#20 epochs with 4 hidden layers, 512 1024 256 64 i got accuracy of 100% starting at 16 epochs, ended with a loss of 0.0004, it is important to note\n",
    "#that it took much longer to train (7 minutes ish)\n",
    "\n",
    "#same excpet config is 1024 2048 512 64 got accuray of 100% on the 16th epoch as well, ended with loss of 0.0003 took same training time\n",
    "\n",
    "#20 epochs hdiden layer sizes of 1024 1536 768 256 64 learning rate of 0.1 took a long time to train () because of extra hidden layer and last layer using sigmoid\n",
    "#anywhere i use sigmoid takes way longer to train after 17 epochs i got 99.94 pretty bad took like 15 - 20 minutes to run\n",
    "\n",
    "# 20 epoch same sizes but all relu got 100% on epoch 14 and loss of 0.0001 at epoch 18 and ended with 0.0001, however, it took 17 minutes to train\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03b0a5ea72e569892ff75cdce4f0a43aa28d5543ecfacea9505a52dbab1ee89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
