{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear() missing 1 required positional arguments: \"weight\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 112>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(outputs, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m#dim = which dimension to appply softmax to i think\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# probs = torch.sigmoid(outputs)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# probs = torch.relu(outputs)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mlinear(outputs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m maxProbs, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(probs, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m#takes highest probability to classify as n number\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# def accuracy (l1, l2): #gives accuracy of predictions\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39m#     return torch.sum(l1 == l2).item() / len(l1)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/WWDC2023/MNISTLogistic.ipynb#W0sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39m#good loss function for this is - sum of actual label natural log probability of what model thought of right label\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear() missing 1 required positional arguments: \"weight\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision #library for working with images\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#matplotlib inline signifies that we want to see output in our ide, not as a popup\n",
    "import torchvision.transforms as transforms #contains predefined functions to convert images to tensors\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler #allows us to sample elements randomly from list of indices but not needed with new shuffle feature\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dataset = MNIST(root = 'data/', download = True) #downloads data set, parameter a is location, b is whther it should be downloaded\n",
    "# print(\"size of training/validation dataset\")\n",
    "# print(len(dataset)) #gives size of dataset\n",
    "\n",
    "testDataset = MNIST(root = 'data/', train = False) #by passing train = false, we signify that we want the other images (the testing ones)\n",
    "# print(\"size of testing set\")\n",
    "# print(len(testDataset))\n",
    "\n",
    "dataset[0] #calls first image in training set\n",
    "\n",
    "#show an individual image\n",
    "image, label, = dataset[0] #deriving the image and the label from the values at index 0 \n",
    "# plt.imshow(image, cmap = 'gray')\n",
    "# print('Label :', label)\n",
    "\n",
    "dataset = MNIST(root = 'data/', train = True, transform = transforms.ToTensor()) #a = data root folder, b = whether it is a training set or not, c = transformation type bc u cant just pass in a raw image\n",
    "\n",
    "#printing image compressed into a tensor\n",
    "imgTensor, label = dataset[0] #deriving the image and the label from the values at index 0\n",
    "# print(imgTensor.shape, label) #print shape of tensor with label, prints out like torch.Size([1, 28, 28]) 5 \n",
    "#first parameter is only one channel since its grayscale, keeps track of color channels, if it was RGB it would be 3, the other 2 are size\n",
    "\n",
    "# print(imgTensor[:, 10:15, 10:15]) #prints a tensor containing values for the part of the image of 10-15x and 10-15 y \n",
    "# print(torch.max(imgTensor), torch.min(imgTensor)) #prints 1 and 0 , 1 is max (white), 0 is low, (black)\n",
    "\n",
    "# plt.imshow(imgTensor[0, 10:15, 10:15], cmap = 'gray') #displays chunk as image, also overrides the other plt image show \n",
    "\n",
    "#training set is used to train model\n",
    "#validation set is used to evaluate model and adjust hyperparameters like the learning rate to pick the best veresion of the model so like while training used to calculate a metric\n",
    "#test set is used to compare different models and report final accuracy\n",
    "\n",
    "def splitIndices(n, valPct): #split data set into validation and training set, n is number of images, valPct is number you want to be validation set\n",
    "    nVal = int(valPct * n) #multiplying to find # of images to make validation\n",
    "    idxs = np.random.permutation(n) #creates a random permutation of n images from 0 to n-1 in the list of images\n",
    "    return idxs[nVal:], idxs[:nVal] #picks the first nVal indices to be used for validation set and returns trianing images and validation images split up and shuffled\n",
    "\n",
    "trainIndices, valIndices = splitIndices(len(dataset), valPct = 0.2) #sets training set and val set to output of function, input length of dataset as n, and valPct of 20 percent\n",
    "\n",
    "#we shuffle because the data might be in order, the only time you want it to be in order is for a problem that has to do with time for example, which needs to be in order\n",
    "\n",
    "#trainIndices should be 48k, valIndices is 12k \n",
    "\n",
    "batchSize = 100 #training in batches is more efficient and allows it to fit in memory if that is a problem\n",
    "\n",
    "#data loaders help us load data into batches\n",
    "#sampler takes indices randomly but instead of using that...\n",
    "# trainLoader = DataLoader(trainIndices, batchSize, shuffle = True) #takes in dataset, the size per epoch, and whether data should be shuffled \n",
    "# valLoader = DataLoader(valIndices, batchSize, shuffle = True)\n",
    "\n",
    "#alternate way of doing this\n",
    "trainSampler = SubsetRandomSampler(trainIndices)\n",
    "trainLoader = DataLoader(dataset, batchSize, sampler = trainSampler)\n",
    "\n",
    "valSampler = SubsetRandomSampler(valIndices)\n",
    "valLoader = DataLoader(dataset, batchSize, sampler = valSampler)\n",
    "\n",
    "inputSize = 28*28 #28*28 pixels\n",
    "numClasses = 10 #0-9\n",
    "\n",
    "model = nn.Linear(inputSize, numClasses) #create model (logistic regression)\n",
    "\n",
    "# print(model.weight.shape)\n",
    "# model.weight\n",
    "\n",
    "# print(model.bias.shape)\n",
    "# model.bias\n",
    "\n",
    "# for images, labels in trainLoader:\n",
    "#     print(labels)\n",
    "#     print(images.shape)\n",
    "#     outputs = model(images) #creates size mitchmatch error expects batch of vectors of size 28*28 but its getting 100*1*28*28 torch.Size([100, 1, 28, 28]) so flatten it into a single vector\n",
    "#     break\n",
    "\n",
    "class MnistModel(nn.Module): #extending nn.Module to reshape into flattened out 28x28 vectors\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(inputSize, numClasses) #instantiatae the weights and biases using nn.Linear\n",
    "\n",
    "    def forward(self, xb): #flattens out input vector and pass into model\n",
    "        xb = xb.reshape(-1, 784) #reshape vector\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "\n",
    "model = MnistModel() #new with reshaped vectors\n",
    "\n",
    "# print(model.linear.weight.shape, model.linear.bias.shape) \n",
    "# print(model.parameters())\n",
    "\n",
    "for images, labels in trainLoader: \n",
    "     outputs = model(images)\n",
    "     break\n",
    "\n",
    "# print('ouputs.shape : ', outputs.shape) its 100, 10 ouputs\n",
    "# print('Sample outputs : \\n', outputs [:2].data) ouputs 10 for each image, probabilities for each thing need to convert to real probabilities tho cuz they just numbers so softmax\n",
    "\n",
    "probs = F.softmax(outputs, dim = 1) #dim = which dimension to appply softmax to i think\n",
    "# probs = torch.sigmoid(outputs)\n",
    "# probs = torch.relu(outputs)\n",
    "probs = F.silu(outputs)\n",
    "\n",
    "maxProbs, preds = torch.max(probs, dim = 1) #takes highest probability to classify as n number\n",
    "\n",
    "# def accuracy (l1, l2): #gives accuracy of predictions\n",
    "#     return torch.sum(l1 == l2).item() / len(l1)\n",
    "\n",
    "#accuracy(preds, labels) output * 100 = number of correct answers out of 100\n",
    "\n",
    "#cannot be used as a loss function as it doesnt take into account probabilities or how close they are, it just takes an ouput of one number \n",
    "\n",
    "#good loss function for this is - sum of actual label natural log probability of what model thought of right label\n",
    "\n",
    "lossFn = F.cross_entropy\n",
    "\n",
    "loss = lossFn(outputs, labels) #output is log of accuracy basically so like 2.3 is like .1 accuracy\n",
    "\n",
    "learningRate = 0.001 #example of hyper parameters\n",
    "optimizer  = torch.optim.SGD(model.parameters(), lr = learningRate) #updates weights and biases during GD\n",
    "\n",
    "#also optionally computes a metric like accuracy\n",
    "def lossBatch(model, lossFunc, xb, yb, opt = None, metric = None): #calculates loss of batch of data optionally pefroms gradient descent update step if optimizer is provided\n",
    "   #calc loss\n",
    "    preds = model(xb) # xb is input tensors into model\n",
    "    loss = lossFunc(preds, yb) \n",
    "\n",
    "    if opt is not None: \n",
    "        loss.backward() #compute gradients\n",
    "        opt.step() #update params\n",
    "        opt.zero_grad()#reset gradients\n",
    "\n",
    "    metricResult = None\n",
    "    if metric is not None:\n",
    "        metricResult = metric(preds, yb) #passes values into metric function, yb is actual labels\n",
    "        \n",
    "    return loss.item(), len(xb), metricResult\n",
    "\n",
    "#calculates overall loss and a metric and also outputs total size of all batches together \n",
    "def evaluate(model, lossFn, validDl, metric = None):\n",
    "    with torch.no_grad(): #dont need to compute gradients with validation set, only for evaluation\n",
    "        results = [lossBatch(model, lossFn, xb, yb, metric = metric) \n",
    "                                    for xb, yb in validDl] #passes each batch through the model\n",
    "        \n",
    "        #seperate\n",
    "        losses, nums, metrics = zip(*results)\n",
    "\n",
    "        #total size is sum of all batch sizes\n",
    "        total = np.sum(nums)\n",
    "\n",
    "        avgLoss = np.sum(np.multiply(losses, nums)) / total #avg loss\n",
    "        avgMetric = None\n",
    "\n",
    "        if metric is not None:\n",
    "            #avg metric of assessment across all batches\n",
    "            avgMetric = np.sum(np.multiply(metrics, nums)) / total\n",
    "    \n",
    "    return avgLoss, total, avgMetric\n",
    "\n",
    "#redefine accuracy to work with an entire batch\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim = 1) #highest value (the one it thinks it is)\n",
    "    return torch.sum(preds == labels).item() / len(preds) #how many are correct\n",
    "\n",
    "#current results before training valLoss, total, valAcc = evaluate(model, lossFn, valLoader, metric = accuracy)\n",
    "\n",
    "#training \n",
    "def fit(epochs, model, lossFn, opt, trainDl, validDl, metric = None):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        #training\n",
    "        for xb, yb in trainDl:\n",
    "            loss,_,_ = lossBatch(model, lossFn, xb, yb, opt)\n",
    "\n",
    "        #evaluation\n",
    "        result = evaluate(model, lossFn, validDl, metric)\n",
    "        valLoss, total, valMetric = result\n",
    "        #prints out progress through training\n",
    "        \n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "            .format(epoch+1, epochs, valLoss))\n",
    "        \n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "            .format(epoch+1, epochs, valLoss, \"Accuracy\", valMetric))\n",
    "\n",
    "fit(20, model, F.cross_entropy, optimizer, trainLoader, valLoader, accuracy)   \n",
    "\n",
    "#after 100 epochs it achieved accuracy of 89.26%. it isnt higher because model isnt that powerful \"The more likely reason that the model just isn't powerful enough. If you remember our initial hypothesis, we have assumed that the output (in this case the class probabilities) is a linear function of the input (pixel intensities), obtained by perfoming a matrix multiplication with the weights matrix and adding the bias. This is a fairly weak assumption, as there may not actually exist a linear relationship between the pixel intensities in an image and the digit it represents. While it works reasonably well for a simple dataset like MNIST (getting us to 85% accuracy), we need more sophisticated models that can capture non-linear relationships between image pixels and labels for complex tasks like recognizing everyday objects, animals etc.\"\n",
    "\n",
    "\n",
    "# doesnt work for some reason      \n",
    "# def predictImage(img, model):\n",
    "#     xb = img.unsqueeze()\n",
    "#     yb = model(xb)\n",
    "#     _, preds = torch.max(yb, dim = 1)\n",
    "#     return preds[0].item()\n",
    "    \n",
    "# img, label = testDataset[0]\n",
    "# plt.imshow(img, cmap = 'gray')\n",
    "# print('Label:', label, ', Predicted:', predictImage(img, model)) s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03b0a5ea72e569892ff75cdce4f0a43aa28d5543ecfacea9505a52dbab1ee89a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
